{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4481a6a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:07.292637Z",
     "start_time": "2021-07-28T08:57:06.216515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import Init_dld as init\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8ded94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:07.338515Z",
     "start_time": "2021-07-28T08:57:07.326586Z"
    }
   },
   "outputs": [],
   "source": [
    "Batch_Size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d1cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training sets has 50793 images, validation sets has 17420 images\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Test sets has 17390 images\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tr_img, tr_lab, val_img, val_lab = init.Load_data(init.data_dir, is_training=True)\n",
    "tst_img, tst_lab = init.Load_data(init.data_dir, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652476f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_img=torch.from_numpy(tr_img)\n",
    "tr_lab=torch.from_numpy(tr_lab)\n",
    "tst_img=torch.from_numpy(tst_img)\n",
    "tst_lab=torch.from_numpy(tst_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ede49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50793, 32, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(tr_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4912e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob,tr_lab=tr_lab.data.max(dim=1)\n",
    "prob,tst_lab=tst_lab.data.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7196259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50793])\n"
     ]
    }
   ],
   "source": [
    "print(tr_lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97154edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_img=tr_img.permute(0, 3, 1, 2)\n",
    "tst_img=tst_img.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c039b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: 'CON',1: 'MUL_GGO',2: 'HCM',3: 'RET_GGO',4: 'EMP',5: 'NOD',6: 'NOR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9addcf45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:09.523745Z",
     "start_time": "2021-07-28T08:57:07.341507Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset=torch.utils.data.TensorDataset(tr_img,tr_lab)\n",
    "testset=torch.utils.data.TensorDataset(tst_img,tst_lab)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=Batch_Size,shuffle=True, num_workers=8)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=Batch_Size,shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e53e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:09.571543Z",
     "start_time": "2021-07-28T08:57:09.561572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50793, 1, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_img.data.shape #50000是图片数量，32x32是图片大小，3是通道数量RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4870dc5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:09.587499Z",
     "start_time": "2021-07-28T08:57:09.575535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "#查看数据类型\n",
    "print(type(tr_img.data))\n",
    "print(type(tr_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b01656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "with open(\"data/DLD_pickle_data/training_image.pkl\", \"rb\") as f:\n",
    "    test_images = pickle.load(f)\n",
    "image_to_show = test_images[2012]\n",
    "plt.imshow(image_to_show)\n",
    "plt.axis('off')  # 关闭坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee2358fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:30.444708Z",
     "start_time": "2021-07-28T08:57:30.064724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # 判断是否用GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28fa0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    对于浅层网络，如ResNet-18/34等，用基本的Block\n",
    "    基础模块没有压缩,所以expansion=1\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels),\n",
    "            )\n",
    "            \n",
    "    def forward(self, x,switch=[]):\n",
    "        halflen=int(len(switch)/2)\n",
    "        out = self.conv1(x)\n",
    "        for i in range(0,halflen):\n",
    "            if(switch[i]==0):\n",
    "                out[:,i,:,:]=float(0)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        for i in range(halflen,len(switch)):\n",
    "            if(switch[i]==0):\n",
    "                out[:,i-halflen,:,:]=0\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87558974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    对于深层网络，我们使用BottleNeck，论文中提出其拥有近似的计算复杂度，但能节省很多资源\n",
    "    zip_channels: 压缩后的维数，最后输出的维数是 expansion * zip_channels\n",
    "    针对ResNet50/101/152的网络结构,主要是因为第三层是第二层的4倍的关系所以expansion=4\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, zip_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        out_channels = self.expansion * zip_channels\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, zip_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(zip_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(zip_channels, zip_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(zip_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(zip_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x,switch=[]):\n",
    "        halflen=int(len(switch)/6)\n",
    "        out = self.conv1(x)\n",
    "        for i in range(0,halflen):\n",
    "            if(switch[i]==0):\n",
    "                out[:,i,:,:]=float(0)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        for i in range(halflen,halflen*2):\n",
    "            if(switch[i]==0):\n",
    "                out[:,i-halflen,:,:]=0\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        for i in range(halflen*2,len(switch)):\n",
    "            if(switch[i]==0):\n",
    "                out[:,i-halflen-halflen,:,:]=0\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cd5eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    不同的ResNet架构都是统一的一层特征提取、四层残差，不同点在于每层残差的深度。\n",
    "    对于cifar10，feature map size的变化如下：\n",
    "    (32, 32, 3) -> [Conv2d] -> (32, 32, 64) -> [Res1] -> (32, 32, 64) -> [Res2] \n",
    "    -> (16, 16, 128) -> [Res3] -> (8, 8, 256) ->[Res4] -> (4, 4, 512) -> [AvgPool] \n",
    "    -> (1, 1, 512) -> [Reshape] -> (512) -> [Linear] -> (10)\n",
    "    \"\"\"\n",
    "    def __init__(self, block, num_blocks, num_classes=7, verbose=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.in_channels = 64\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.out1=64\n",
    "        self.out2=128\n",
    "        self.out3=256\n",
    "        self.out4=512\n",
    "        # 使用_make_layer函数生成上表对应的conv2_x, conv3_x, conv4_x, conv5_x的结构\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # cifar10经过上述结构后，到这里的feature map size是 4 x 4 x 512 x expansion\n",
    "        # 所以这里用了 4 x 4 的平均池化\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n",
    "        self.classifer = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        # 第一个block要进行降采样\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = nn.ModuleList()\n",
    "        for stride in strides:\n",
    "            layer = block(self.in_channels, out_channels, stride)\n",
    "            layers.append(layer)\n",
    "            # 如果是Bottleneck Block的话需要对每层输入的维度进行压缩，压缩后再增加维数\n",
    "            # 所以每层的输入维数也要跟着变\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x, switch=[1] * 3776*2):\n",
    "        out = self.features(x)\n",
    "        if self.verbose:\n",
    "            print('block 1 output: {}'.format(out.shape))\n",
    "        \n",
    "        i=0\n",
    "\n",
    "        for layer in self.layer1:\n",
    "                out = layer(out,switch[i:i+self.out1*2])\n",
    "                i = i+self.out1*2\n",
    "        if self.verbose:\n",
    "            print('block 2 output: {}'.format(out.shape))\n",
    "\n",
    "        for layer in self.layer2:\n",
    "            out = layer(out,switch[i:i+self.out2*2])\n",
    "            i = i+self.out2*2\n",
    "        if self.verbose:\n",
    "            print('block 3 output: {}'.format(out.shape))\n",
    "\n",
    "        for layer in self.layer3:\n",
    "            out = layer(out,switch[i:i+self.out3*2])\n",
    "            i = i+self.out3*2\n",
    "        if self.verbose:\n",
    "            print('block 4 output: {}'.format(out.shape))\n",
    "            \n",
    "        for layer in self.layer4:\n",
    "            out = layer(out,switch[i:i+self.out4*2])\n",
    "            i = i+self.out4*2\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('block 5 output: {}'.format(out.shape))\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifer(out)\n",
    "        i=0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b956976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResNet2(nn.Module):\n",
    "    \"\"\"\n",
    "    不同的ResNet架构都是统一的一层特征提取、四层残差，不同点在于每层残差的深度。\n",
    "    对于cifar10，feature map size的变化如下：\n",
    "    (32, 32, 3) -> [Conv2d] -> (32, 32, 64) -> [Res1] -> (32, 32, 64) -> [Res2] \n",
    "    -> (16, 16, 128) -> [Res3] -> (8, 8, 256) ->[Res4] -> (4, 4, 512) -> [AvgPool] \n",
    "    -> (1, 1, 512) -> [Reshape] -> (512) -> [Linear] -> (10)\n",
    "    \"\"\"\n",
    "    def __init__(self, block, num_blocks, num_classes=7, verbose=False):\n",
    "        super(ResNet2, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.in_channels = 64\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.out1=64\n",
    "        self.out2=128\n",
    "        self.out3=256\n",
    "        self.out4=512\n",
    "        # 使用_make_layer函数生成上表对应的conv2_x, conv3_x, conv4_x, conv5_x的结构\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # cifar10经过上述结构后，到这里的feature map size是 4 x 4 x 512 x expansion\n",
    "        # 所以这里用了 4 x 4 的平均池化\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n",
    "        self.classifer = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        # 第一个block要进行降采样\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = nn.ModuleList()\n",
    "        for stride in strides:\n",
    "            layer = block(self.in_channels, out_channels, stride)\n",
    "            layers.append(layer)\n",
    "            # 如果是Bottleneck Block的话需要对每层输入的维度进行压缩，压缩后再增加维数\n",
    "            # 所以每层的输入维数也要跟着变\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x, switch=[1] * 22656):\n",
    "        out = self.features(x)\n",
    "        if self.verbose:\n",
    "            print('block 1 output: {}'.format(out.shape))\n",
    "        \n",
    "        i=0\n",
    "        for layer in self.layer1:\n",
    "                out = layer(out,switch[i:i+self.out1*6])\n",
    "                i = i+self.out1*6\n",
    "        if self.verbose:\n",
    "            print('block 2 output: {}'.format(out.shape))\n",
    "\n",
    "        for layer in self.layer2:\n",
    "            out = layer(out,switch[i:i+self.out2*6])\n",
    "            i = i+self.out2*6\n",
    "        if self.verbose:\n",
    "            print('block 3 output: {}'.format(out.shape))\n",
    "\n",
    "        for layer in self.layer3:\n",
    "            out = layer(out,switch[i:i+self.out3*6])\n",
    "            i = i+self.out3*6\n",
    "        if self.verbose:\n",
    "            print('block 4 output: {}'.format(out.shape))\n",
    "            \n",
    "        for layer in self.layer4:\n",
    "            out = layer(out,switch[i:i+self.out4*6])\n",
    "            i = i+self.out4*6\n",
    "        if self.verbose:\n",
    "            print('block 5 output: {}'.format(out.shape))\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifer(out)\n",
    "        i=0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19657e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:34.087990Z",
     "start_time": "2021-07-28T08:57:30.772830Z"
    }
   },
   "outputs": [],
   "source": [
    "def ResNet18(verbose=False):\n",
    "    return ResNet(BasicBlock, [2,2,2,2],verbose=verbose)\n",
    "\n",
    "def ResNet34(verbose=False):\n",
    "    return ResNet(BasicBlock, [3,4,6,3],verbose=verbose)\n",
    "\n",
    "def ResNet50(verbose=False):\n",
    "    return ResNet2(Bottleneck, [3,4,6,3],verbose=verbose)\n",
    "\n",
    "def ResNet101(verbose=False):\n",
    "    return ResNet2(Bottleneck, [3,4,23,3],verbose=verbose)\n",
    "\n",
    "def ResNet152(verbose=False):\n",
    "    return ResNet2(Bottleneck, [3,8,36,3],verbose=verbose)\n",
    "\n",
    "#net = ResNet18(True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e583234f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T08:57:37.504822Z",
     "start_time": "2021-07-28T08:57:36.999174Z"
    }
   },
   "outputs": [],
   "source": [
    "net = ResNet34().to(device)\n",
    "if device == 'cuda':\n",
    "    net = nn.DataParallel(net)\n",
    "    # 当计算图不会改变的时候（每次输入形状相同，模型不改变）的情况下可以提高性能，反之则降低性能\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb7f33f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T11:43:17.184106Z",
     "start_time": "2021-07-27T11:43:17.169147Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5,verbose=True,patience = 5,min_lr = 0.000001) # 动态更新学习率\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 150], gamma=0.5)\n",
    "\n",
    "import time\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e286c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T11:43:17.200065Z",
     "start_time": "2021-07-27T11:43:17.188099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已存在\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "else:\n",
    "    print('文件已存在')\n",
    "save_path = './model/ResNettest.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ce43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/100:   0%|          | 0/199 [00:00<?, ?it/s<class 'dict'>]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/100:  62%|██████▏   | 124/199 [00:22<00:08,  8.45it/s, Train Acc=0.68, Train Loss=1.23] "
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "from utils import plot_history\n",
    "Acc, Loss, Lr = train(net, trainloader, testloader, epoch, optimizer, criterion, scheduler, save_path, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer1): ModuleList(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): ModuleList(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): ModuleList(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer4): ModuleList(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    (classifer): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e3d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (classifer): Linear(in_features=2048, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "net=torch.load(\"model/ResNet50.pth\")\n",
    "\n",
    "net=net.module\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(ori_acc):\n",
    "    def eval_func(state_switch):\n",
    "        #new_acc,_ = validation_procedure(val_img=val_img,val_lab=val_lab,switch=state_switch)\n",
    "        new_acc = float(test(switch=state_switch))\n",
    "        #print(new_acc)\n",
    "        '''if caonima > 0.9478:\n",
    "            drop_num= ' '.join([str(i) for i in state_switch])\n",
    "            save2file(drop_num)'''\n",
    "        return new_acc/ori_acc\n",
    "    return eval_func\n",
    "\n",
    "def test(switch=[]):\n",
    "    correct = 0   # 定义预测正确的图片数，初始化为0\n",
    "    total = 0     # 总共参与测试的图片数，也初始化为0\n",
    "    torch.cuda.empty_cache()\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:  # 循环每一个batch\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            net.eval()  # 把模型转为test模式\n",
    "            if hasattr(torch.cuda, 'empty_cache'):\n",
    "                torch.cuda.empty_cache()\n",
    "            outputs = net(images,switch)  # 输入网络进行测试\n",
    "\n",
    "        # outputs.data是一个4x10张量，将每一行的最大的那一列的值和序号各自组成一个一维张量返回，第一个是值的张量，第二个是序号的张量。\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)          # 更新测试图片的数量\n",
    "            correct += (predicted == labels).sum() # 更新正确分类的图片的数量\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cb4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning procedure :\n",
      "Remain 22656\n",
      "mutation interval: [7, 14]\n",
      "init pop\n",
      "  Best Fitness: 0.9756\n",
      "  Pop Fitness: [0.7814, 0.9671, 0.8778, 0.8302, 0.9185, 0.8229, 0.7267, 0.7984, 0.8733, 0.4829, 0.7951, 0.9533, 0.8323, 0.8015, 0.6988, 0.6914, 0.8586, 0.759, 0.9174, 0.8284, 0.782, 0.7741, 0.8753, 0.9006, 0.7876, 0.7143, 0.842, 0.7886, 0.7527, 0.9756]\n",
      "1 evolution\n",
      "    survival: [29, 1, 11, 4, 18]\n",
      "    obsolete: [0, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 0.9770\n",
      "  Pop Fitness: [0.9543, 0.9671, 0.9156, 0.9663, 0.9185, 0.9641, 0.9163, 0.975, 0.9201, 0.9536, 0.9654, 0.9533, 0.977, 0.8193, 0.8628, 0.8881, 0.956, 0.8152, 0.9174, 0.8752, 0.9569, 0.9406, 0.7857, 0.8474, 0.9184, 0.904, 0.9608, 0.9041, 0.8984, 0.9756]\n",
      "  time use: 188.44996571540833\n",
      "2 evolution\n",
      "    survival: [12, 29, 7, 1, 3]\n",
      "    obsolete: [0, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 0.9775\n",
      "  Pop Fitness: [0.9748, 0.9671, 0.9762, 0.9663, 0.9666, 0.9758, 0.9775, 0.975, 0.9751, 0.968, 0.9768, 0.9663, 0.977, 0.9709, 0.96, 0.9314, 0.954, 0.9584, 0.9154, 0.9437, 0.9314, 0.9337, 0.9518, 0.8999, 0.9686, 0.9576, 0.9394, 0.9707, 0.9348, 0.9756]\n",
      "  time use: 188.46549367904663\n",
      "3 evolution\n",
      "    survival: [6, 12, 10, 2, 5]\n",
      "    obsolete: [0, 1, 3, 4, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 0.9809\n",
      "  Pop Fitness: [0.9809, 0.9762, 0.9762, 0.9774, 0.9752, 0.9758, 0.9775, 0.9752, 0.9755, 0.9772, 0.9768, 0.9756, 0.977, 0.9755, 0.9755, 0.9758, 0.9762, 0.9775, 0.9774, 0.975, 0.9758, 0.9772, 0.9775, 0.9768, 0.9772, 0.9769, 0.977, 0.9753, 0.977, 0.9772]\n",
      "  time use: 188.69655776023865\n",
      "4 evolution\n",
      "    survival: [0, 17, 6, 22, 3]\n",
      "    obsolete: [1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 0.9820\n",
      "  Pop Fitness: [0.9809, 0.9768, 0.977, 0.9774, 0.9771, 0.9805, 0.9775, 0.9769, 0.971, 0.9785, 0.9782, 0.9749, 0.9806, 0.9758, 0.9767, 0.9758, 0.9756, 0.9775, 0.9768, 0.9772, 0.9804, 0.977, 0.9775, 0.9774, 0.9766, 0.9768, 0.982, 0.977, 0.9806, 0.9772]\n",
      "  time use: 187.27814149856567\n",
      "5 evolution\n",
      "    survival: [26, 0, 12, 28, 5]\n",
      "    obsolete: [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 0.9828\n",
      "  Pop Fitness: [0.9809, 0.9806, 0.9218, 0.9806, 0.9801, 0.9805, 0.9828, 0.9813, 0.9808, 0.9808, 0.9808, 0.9801, 0.9806, 0.9813, 0.981, 0.9803, 0.9822, 0.9782, 0.9814, 0.9815, 0.9805, 0.9815, 0.9804, 0.9808, 0.9804, 0.9815, 0.982, 0.9808, 0.9806, 0.9819]\n",
      "  time use: 189.18986296653748\n",
      "6 evolution\n",
      "    survival: [6, 16, 26, 29, 21]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 27, 28]\n",
      "  Best Fitness: 0.9832\n",
      "  Pop Fitness: [0.9821, 0.9813, 0.9819, 0.9559, 0.9817, 0.9826, 0.9828, 0.9762, 0.982, 0.9827, 0.9817, 0.9817, 0.9814, 0.9821, 0.9829, 0.982, 0.9822, 0.9819, 0.9814, 0.9817, 0.9818, 0.9815, 0.982, 0.9817, 0.981, 0.9832, 0.982, 0.9817, 0.9822, 0.9819]\n",
      "  time use: 188.48480939865112\n",
      "7 evolution\n",
      "    survival: [25, 14, 6, 9, 5]\n",
      "    obsolete: [0, 1, 2, 3, 4, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 0.9854\n",
      "  Pop Fitness: [0.9854, 0.9827, 0.9829, 0.9826, 0.9826, 0.9826, 0.9828, 0.9818, 0.9832, 0.9827, 0.9825, 0.9817, 0.9826, 0.9827, 0.9829, 0.982, 0.983, 0.9825, 0.9817, 0.9826, 0.9824, 0.9837, 0.9825, 0.9829, 0.9845, 0.9832, 0.9827, 0.9826, 0.9839, 0.9839]\n",
      "  time use: 187.46709418296814\n",
      "8 evolution\n",
      "    survival: [0, 24, 28, 29, 21]\n",
      "    obsolete: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 27]\n",
      "  Best Fitness: 0.9879\n",
      "  Pop Fitness: [0.9854, 0.9853, 0.9858, 0.9879, 0.9853, 0.9844, 0.9841, 0.9854, 0.984, 0.9837, 0.9834, 0.9829, 0.9838, 0.9827, 0.9859, 0.9853, 0.983, 0.9839, 0.9852, 0.9832, 0.9827, 0.9837, 0.987, 0.9867, 0.9845, 0.9839, 0.9833, 0.9868, 0.9839, 0.9839]\n",
      "  time use: 193.43255925178528\n",
      "9 evolution\n",
      "    survival: [3, 22, 27, 23, 14]\n",
      "    obsolete: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 28, 29]\n",
      "  Best Fitness: 0.9918\n",
      "  Pop Fitness: [0.986, 0.9888, 0.9868, 0.9879, 0.9876, 0.9915, 0.9868, 0.9871, 0.9864, 0.9842, 0.9918, 0.9904, 0.9851, 0.9856, 0.9859, 0.9867, 0.9865, 0.9903, 0.9894, 0.9864, 0.9853, 0.9863, 0.987, 0.9867, 0.987, 0.9867, 0.9865, 0.9868, 0.9901, 0.9861]\n",
      "  time use: 192.65395593643188\n",
      "10 evolution\n",
      "    survival: [10, 5, 11, 17, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 0.9959\n",
      "  Pop Fitness: [0.9905, 0.9959, 0.9905, 0.9916, 0.9917, 0.9915, 0.9899, 0.9806, 0.9906, 0.9935, 0.9918, 0.9904, 0.9905, 0.9953, 0.9921, 0.9901, 0.9917, 0.9903, 0.9914, 0.9901, 0.9897, 0.9908, 0.9902, 0.9913, 0.9918, 0.9914, 0.9896, 0.9852, 0.9901, 0.9904]\n",
      "  time use: 192.64047980308533\n",
      "11 evolution\n",
      "    survival: [1, 13, 9, 14, 24]\n",
      "    obsolete: [0, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 0.9966\n",
      "  Pop Fitness: [0.9955, 0.9959, 0.9926, 0.9953, 0.9915, 0.9966, 0.99, 0.9927, 0.9867, 0.9935, 0.992, 0.9941, 0.9965, 0.9953, 0.9921, 0.9931, 0.9919, 0.9891, 0.9935, 0.9935, 0.9915, 0.9954, 0.9912, 0.9915, 0.9918, 0.9953, 0.9917, 0.9951, 0.9916, 0.9922]\n",
      "  time use: 192.80946373939514\n",
      "12 evolution\n",
      "    survival: [5, 12, 1, 0, 21]\n",
      "    obsolete: [2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 0.9978\n",
      "  Pop Fitness: [0.9955, 0.9959, 0.9968, 0.9968, 0.9961, 0.9966, 0.9956, 0.9954, 0.9933, 0.9963, 0.9975, 0.9967, 0.9965, 0.9842, 0.9957, 0.9922, 0.9961, 0.995, 0.9963, 0.9832, 0.9963, 0.9954, 0.9932, 0.9956, 0.9978, 0.9978, 0.9945, 0.9967, 0.9978, 0.996]\n",
      "  time use: 191.03327655792236\n",
      "13 evolution\n",
      "    survival: [24, 25, 28, 10, 3]\n",
      "    obsolete: [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 27, 29]\n",
      "  Best Fitness: 1.0001\n",
      "  Pop Fitness: [0.9981, 0.9886, 0.9975, 0.9968, 0.9959, 0.9969, 0.9976, 0.9961, 0.9971, 0.9954, 0.9975, 0.9962, 0.9985, 0.9961, 0.9963, 0.9938, 0.9989, 0.9978, 0.9973, 0.9976, 0.9966, 0.9966, 1.0001, 0.9943, 0.9978, 0.9978, 0.9982, 0.9997, 0.9978, 0.998]\n",
      "  time use: 194.50004768371582\n",
      "14 evolution\n",
      "    survival: [22, 27, 16, 12, 26]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 17, 18, 19, 20, 21, 23, 24, 25, 28, 29]\n",
      "  Best Fitness: 1.0017\n",
      "  Pop Fitness: [0.9989, 0.9987, 1.0005, 1.0003, 0.9984, 1.0004, 0.875, 0.9981, 0.9985, 0.9965, 0.9983, 0.9981, 0.9985, 0.9984, 0.9988, 0.9984, 0.9989, 0.9995, 0.9975, 0.9995, 0.9984, 0.9978, 1.0001, 1.0015, 0.9966, 0.9945, 0.9982, 0.9997, 1.0017, 0.9944]\n",
      "  time use: 194.38286519050598\n",
      "15 evolution\n",
      "    survival: [28, 23, 2, 5, 3]\n",
      "    obsolete: [0, 1, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0038\n",
      "  Pop Fitness: [1.0016, 1.0011, 1.0005, 1.0003, 1.0, 1.0004, 0.9999, 1.0, 0.9997, 0.9977, 1.0009, 1.0009, 1.0008, 0.9991, 1.0004, 1.0008, 1.0025, 1.0016, 1.0008, 1.0001, 0.9994, 1.0001, 0.9944, 1.0015, 1.0038, 1.0037, 1.0003, 1.002, 1.0017, 1.0015]\n",
      "  time use: 192.11023998260498\n",
      "16 evolution\n",
      "    survival: [24, 25, 16, 27, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 26, 29]\n",
      "  Best Fitness: 1.0050\n",
      "  Pop Fitness: [0.9979, 1.0008, 1.0018, 0.976, 1.0009, 1.0026, 1.0048, 1.0028, 0.9991, 1.0018, 1.0037, 1.0011, 1.0023, 1.0006, 0.9953, 1.0041, 1.0025, 1.0025, 1.0039, 1.002, 1.003, 1.0007, 1.005, 1.0039, 1.0038, 1.0037, 1.003, 1.002, 1.0017, 1.0028]\n",
      "  time use: 193.17791271209717\n",
      "17 evolution\n",
      "    survival: [22, 6, 15, 18, 23]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0056\n",
      "  Pop Fitness: [1.0048, 1.0035, 1.0018, 1.0022, 0.9975, 1.0015, 1.0048, 1.0036, 1.0043, 1.0049, 1.002, 1.0046, 1.0032, 1.0051, 1.0029, 1.0041, 1.0049, 1.0051, 1.0039, 1.0041, 1.0056, 1.0016, 1.005, 1.0039, 1.0049, 1.0038, 1.0038, 1.0052, 1.0048, 1.0047]\n",
      "  time use: 193.5897936820984\n",
      "18 evolution\n",
      "    survival: [20, 27, 13, 17, 22]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0059\n",
      "  Pop Fitness: [1.0048, 1.0038, 1.0052, 1.0052, 1.0059, 1.0013, 1.0048, 1.0051, 1.0051, 1.0051, 0.9975, 1.0045, 1.0051, 1.0051, 1.0051, 1.0049, 1.0053, 1.0051, 1.0018, 1.0055, 1.0056, 1.0039, 1.005, 1.0054, 1.0035, 1.005, 1.0027, 1.0052, 1.0027, 1.0052]\n",
      "  time use: 192.8784339427948\n",
      "19 evolution\n",
      "    survival: [4, 20, 19, 23, 16]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 21, 22, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0071\n",
      "  Pop Fitness: [1.0053, 1.0061, 1.0052, 1.0053, 1.0059, 1.0002, 1.0062, 1.0054, 1.0043, 1.0052, 1.0054, 1.0049, 1.0054, 1.0006, 1.0037, 1.0071, 1.0053, 1.0053, 1.0052, 1.0055, 1.0056, 1.0054, 1.0051, 1.0054, 1.0051, 1.0054, 1.0064, 1.0058, 1.0065, 1.004]\n",
      "  time use: 192.20234322547913\n",
      "20 evolution\n",
      "    survival: [15, 28, 26, 6, 1]\n",
      "    obsolete: [0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 1.0095\n",
      "  Pop Fitness: [1.0046, 1.0061, 1.006, 1.0056, 1.0066, 1.0057, 1.0062, 1.0043, 1.0057, 1.0058, 1.0061, 1.0062, 1.0046, 1.0063, 1.0068, 1.0071, 1.0063, 1.0066, 1.0061, 1.007, 1.0095, 1.0039, 1.0065, 1.0068, 1.0046, 1.0063, 1.0064, 1.0059, 1.0065, 1.0079]\n",
      "  time use: 194.04523348808289\n",
      "21 evolution\n",
      "    survival: [20, 29, 15, 19, 23]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 21, 22, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0101\n",
      "  Pop Fitness: [1.0085, 1.0078, 1.0068, 0.9988, 1.0074, 1.0069, 1.0072, 1.007, 1.0068, 0.9842, 1.0082, 1.0049, 1.007, 1.0073, 1.0101, 1.0071, 1.0073, 1.0077, 1.0089, 1.007, 1.0095, 1.0046, 1.0068, 1.0068, 1.007, 1.0067, 1.0074, 1.008, 1.006, 1.0079]\n",
      "  time use: 193.26175737380981\n",
      "22 evolution\n",
      "    survival: [14, 20, 18, 0, 10]\n",
      "    obsolete: [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0108\n",
      "  Pop Fitness: [1.0085, 1.0082, 1.008, 1.0083, 1.0102, 1.0082, 1.0095, 1.0088, 1.007, 0.9788, 1.0082, 1.0108, 1.0099, 1.0084, 1.0101, 1.009, 1.0097, 1.0092, 1.0089, 1.0091, 1.0095, 1.0102, 1.0085, 1.0094, 1.0082, 1.0104, 1.0101, 1.007, 1.0091, 1.0073]\n",
      "  time use: 191.7068064212799\n",
      "23 evolution\n",
      "    survival: [11, 25, 4, 21, 26]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0144\n",
      "  Pop Fitness: [1.0094, 1.0096, 1.0102, 1.013, 1.0102, 1.0047, 1.0101, 1.0101, 1.0104, 1.0097, 1.0098, 1.0108, 1.0101, 1.0094, 1.0097, 1.0118, 1.0077, 1.0099, 1.0144, 1.009, 1.0109, 1.0102, 1.0097, 1.0101, 1.0111, 1.0104, 1.0101, 1.0098, 1.0108, 1.009]\n",
      "  time use: 194.2412075996399\n",
      "24 evolution\n",
      "    survival: [18, 3, 15, 24, 20]\n",
      "    obsolete: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0144\n",
      "  Pop Fitness: [1.0105, 1.013, 1.0126, 1.013, 1.0105, 1.0106, 1.0128, 1.0105, 1.013, 1.0108, 1.0129, 1.0129, 1.0139, 1.0109, 1.0108, 1.0118, 1.0108, 1.0106, 1.0144, 1.0109, 1.0109, 1.0099, 1.0124, 1.0132, 1.0111, 1.0101, 1.0121, 1.0134, 1.0123, 1.0109]\n",
      "  time use: 193.69423580169678\n",
      "25 evolution\n",
      "    survival: [18, 12, 27, 23, 1]\n",
      "    obsolete: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0154\n",
      "  Pop Fitness: [1.0139, 1.013, 0.8593, 1.0127, 1.0145, 1.0147, 1.013, 1.0139, 1.0137, 1.0119, 1.0143, 1.0133, 1.0139, 1.0137, 1.0139, 1.0137, 1.0128, 1.0134, 1.0144, 1.0133, 1.0137, 1.0093, 1.0154, 1.0132, 1.0126, 1.013, 1.0131, 1.0134, 1.0133, 1.0146]\n",
      "  time use: 192.4383237361908\n",
      "26 evolution\n",
      "    survival: [22, 5, 29, 4, 18]\n",
      "    obsolete: [0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0168\n",
      "  Pop Fitness: [1.0145, 1.0168, 1.0131, 1.0137, 1.0145, 1.0147, 1.014, 1.0144, 1.0138, 1.0137, 1.014, 1.0089, 1.0147, 1.0138, 0.9897, 1.0147, 1.0137, 1.0149, 1.0144, 1.0144, 1.015, 1.0144, 1.0154, 1.0125, 1.0148, 1.0144, 1.0145, 1.0135, 1.0149, 1.0146]\n",
      "  time use: 195.46510553359985\n",
      "27 evolution\n",
      "    survival: [1, 22, 20, 17, 28]\n",
      "    obsolete: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0175\n",
      "  Pop Fitness: [1.0138, 1.0168, 1.0168, 0.9974, 1.0132, 1.0169, 1.0132, 0.9003, 1.0175, 1.0147, 1.014, 1.0151, 1.0154, 1.0157, 1.0168, 1.0169, 1.0152, 1.0149, 1.0148, 1.0161, 1.015, 1.0062, 1.0154, 1.0089, 1.0145, 1.0153, 1.0171, 1.0142, 1.0149, 1.016]\n",
      "  time use: 193.42093181610107\n",
      "28 evolution\n",
      "    survival: [8, 26, 15, 5, 1]\n",
      "    obsolete: [0, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29]\n",
      "  Best Fitness: 1.0178\n",
      "  Pop Fitness: [1.0141, 1.0168, 1.0114, 1.0161, 1.0176, 1.0169, 1.0166, 1.0169, 1.0175, 1.0166, 1.0135, 1.0176, 1.017, 1.0168, 1.0165, 1.0169, 1.0166, 1.0144, 1.0158, 1.0161, 1.0178, 1.0168, 1.0163, 1.0175, 1.0165, 1.0169, 1.0171, 1.0172, 1.0171, 1.0176]\n",
      "  time use: 193.9316577911377\n",
      "29 evolution\n",
      "    survival: [20, 4, 11, 29, 8]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0179\n",
      "  Pop Fitness: [1.0171, 1.0161, 1.0169, 1.0177, 1.0176, 1.0071, 1.0151, 1.0161, 1.0175, 1.0179, 1.0148, 1.0176, 1.0178, 1.0076, 1.0172, 1.017, 1.0175, 1.0169, 1.0159, 1.0154, 1.0178, 1.0167, 1.0164, 1.0176, 1.0171, 1.0171, 1.0168, 1.0175, 1.0169, 1.0176]\n",
      "  time use: 194.04196071624756\n",
      "30 evolution\n",
      "    survival: [9, 12, 20, 3, 4]\n",
      "    obsolete: [0, 1, 2, 5, 6, 7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0186\n",
      "  Pop Fitness: [1.0183, 1.0179, 1.0164, 1.0177, 1.0176, 1.0159, 1.0167, 1.0164, 1.0171, 1.0179, 1.0173, 1.0116, 1.0178, 1.0181, 1.0181, 1.018, 1.0168, 1.0116, 1.0141, 1.0184, 1.0178, 1.0163, 1.0178, 1.0152, 1.0178, 1.0171, 1.0186, 1.0179, 1.0176, 1.0173]\n",
      "  time use: 195.88269138336182\n",
      "31 evolution\n",
      "    survival: [26, 19, 0, 13, 14]\n",
      "    obsolete: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 27, 28, 29]\n",
      "  Best Fitness: 1.0190\n",
      "  Pop Fitness: [1.0183, 1.0104, 1.0175, 1.0186, 1.0187, 0.9985, 1.0182, 1.0163, 1.0178, 1.018, 1.0161, 1.0179, 1.0156, 1.0181, 1.0181, 1.017, 1.0171, 1.017, 1.0179, 1.0184, 1.0003, 1.0162, 1.0178, 1.0159, 1.0183, 1.0182, 1.0186, 1.0179, 1.0169, 1.019]\n",
      "  time use: 197.01945662498474\n",
      "32 evolution\n",
      "    survival: [29, 4, 3, 26, 19]\n",
      "    obsolete: [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 27, 28]\n",
      "  Best Fitness: 1.0193\n",
      "  Pop Fitness: [1.018, 1.0188, 1.0189, 1.0186, 1.0187, 1.0178, 1.019, 1.0053, 1.0186, 1.0186, 1.0177, 1.0183, 1.0187, 1.0176, 1.0187, 1.0173, 1.0181, 1.018, 1.0176, 1.0184, 1.018, 1.0193, 1.0188, 1.0191, 1.0153, 1.0181, 1.0186, 1.0182, 1.019, 1.019]\n",
      "  time use: 198.66552066802979\n",
      "33 evolution\n",
      "    survival: [21, 23, 6, 29, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 24, 25, 26, 27]\n",
      "  Best Fitness: 1.0193\n",
      "  Pop Fitness: [1.018, 1.0183, 1.0192, 1.0175, 1.0011, 1.0189, 1.019, 1.0188, 1.018, 1.018, 1.0192, 1.0186, 1.0187, 1.0149, 1.0186, 1.018, 1.0188, 1.0192, 1.0188, 1.019, 1.0189, 1.0193, 1.0182, 1.0191, 1.019, 1.0185, 1.0182, 1.0164, 1.019, 1.019]\n",
      "  time use: 199.3267912864685\n",
      "34 evolution\n",
      "    survival: [21, 2, 10, 17, 23]\n",
      "    obsolete: [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0211\n",
      "  Pop Fitness: [1.0178, 1.0211, 1.0192, 1.0183, 1.0149, 1.0068, 1.0187, 1.0188, 1.0192, 1.0209, 1.0192, 1.0197, 1.0169, 1.019, 1.0178, 1.0171, 1.0194, 1.0192, 1.0194, 1.0052, 1.0184, 1.0193, 1.0135, 1.0191, 1.0185, 1.0194, 1.0194, 1.0192, 1.0183, 1.0184]\n",
      "  time use: 198.511803150177\n",
      "35 evolution\n",
      "    survival: [1, 9, 11, 16, 18]\n",
      "    obsolete: [0, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 14, 15, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0230\n",
      "  Pop Fitness: [1.02, 1.0211, 1.0207, 1.0197, 1.0216, 1.0186, 1.02, 1.0193, 1.0209, 1.0209, 1.0183, 1.0197, 1.023, 1.0158, 1.0204, 1.0199, 1.0194, 1.0207, 1.0194, 1.0206, 1.0223, 1.0155, 1.0085, 1.0175, 1.0209, 1.0185, 1.0182, 1.0189, 1.0185, 1.0195]\n",
      "  time use: 195.20057845115662\n",
      "36 evolution\n",
      "    survival: [12, 20, 4, 1, 8]\n",
      "    obsolete: [0, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0230\n",
      "  Pop Fitness: [1.019, 1.0211, 1.022, 1.0169, 1.0216, 1.0148, 1.0198, 1.018, 1.0209, 1.0211, 1.0206, 1.0209, 1.023, 1.0227, 1.0229, 1.0209, 1.0225, 1.0209, 1.0218, 1.022, 1.0223, 1.0214, 1.0225, 1.023, 1.021, 1.0212, 1.0214, 1.0189, 1.0198, 1.0192]\n",
      "  time use: 194.20384287834167\n",
      "37 evolution\n",
      "    survival: [23, 12, 14, 13, 16]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0240\n",
      "  Pop Fitness: [1.0222, 1.0206, 1.0231, 1.0226, 1.0233, 1.0232, 1.0228, 1.0218, 1.024, 1.0175, 1.0218, 1.019, 1.023, 1.0227, 1.0229, 1.0237, 1.0225, 1.0225, 1.0223, 1.0226, 1.022, 1.0223, 1.0231, 1.023, 1.0211, 1.0233, 1.0229, 1.0233, 1.0216, 1.0228]\n",
      "  time use: 194.72030305862427\n",
      "38 evolution\n",
      "    survival: [8, 15, 4, 25, 27]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 28, 29]\n",
      "  Best Fitness: 1.0247\n",
      "  Pop Fitness: [1.0144, 1.0237, 1.0226, 1.0233, 1.0233, 1.0231, 1.0223, 1.0224, 1.024, 1.0199, 1.0212, 1.0226, 1.0233, 1.0214, 1.0242, 1.0237, 1.0243, 1.0226, 1.0221, 1.0156, 1.0227, 1.0195, 1.0233, 1.0235, 1.0232, 1.0233, 1.0247, 1.0233, 1.024, 1.0229]\n",
      "  time use: 193.89295864105225\n",
      "39 evolution\n",
      "    survival: [26, 16, 14, 8, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 1.0247\n",
      "  Pop Fitness: [1.0188, 1.024, 1.024, 1.0231, 1.0241, 1.0231, 1.0227, 1.0242, 1.024, 1.0237, 1.0235, 1.0194, 1.0242, 1.0227, 1.0242, 1.0241, 1.0243, 1.0236, 1.024, 1.0237, 0.9627, 1.024, 1.0242, 1.023, 1.024, 1.0228, 1.0247, 1.0246, 1.024, 1.0247]\n",
      "  time use: 194.66927480697632\n",
      "40 evolution\n",
      "    survival: [26, 29, 27, 16, 7]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28]\n",
      "  Best Fitness: 1.0254\n",
      "  Pop Fitness: [1.0211, 1.0125, 1.0241, 1.0242, 1.0244, 1.0245, 1.0228, 1.0242, 1.024, 1.024, 1.0243, 1.0247, 1.0245, 1.0227, 1.0211, 1.0237, 1.0243, 1.0254, 1.0195, 1.0242, 1.0242, 1.0247, 1.0237, 1.0247, 1.0244, 1.0238, 1.0247, 1.0246, 1.0246, 1.0247]\n",
      "  time use: 193.7986340522766\n",
      "41 evolution\n",
      "    survival: [17, 26, 29, 11, 21]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 25, 27, 28]\n",
      "  Best Fitness: 1.0258\n",
      "  Pop Fitness: [1.0243, 1.0258, 1.0237, 1.0241, 1.0249, 1.0255, 1.018, 1.0245, 1.024, 1.0239, 1.0235, 1.0247, 1.0168, 1.0235, 1.025, 1.0233, 1.0221, 1.0254, 1.0246, 1.0244, 1.0244, 1.0247, 1.0249, 1.0237, 1.0243, 1.0245, 1.0247, 1.0243, 1.0249, 1.0247]\n",
      "  time use: 193.4342782497406\n",
      "42 evolution\n",
      "    survival: [1, 5, 17, 14, 4]\n",
      "    obsolete: [0, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0262\n",
      "  Pop Fitness: [1.0254, 1.0258, 1.0247, 1.0197, 1.0249, 1.0255, 1.0225, 1.0227, 1.0242, 1.0238, 1.025, 1.02, 1.0187, 1.025, 1.025, 1.0249, 1.0245, 1.0254, 1.0231, 1.0217, 1.0236, 1.0242, 1.0262, 1.0233, 1.0254, 1.0254, 1.0253, 1.0249, 1.0252, 1.0252]\n",
      "  time use: 193.55308985710144\n",
      "43 evolution\n",
      "    survival: [22, 1, 5, 24, 25]\n",
      "    obsolete: [0, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0263\n",
      "  Pop Fitness: [1.0247, 1.0258, 1.0231, 1.0228, 1.0253, 1.0255, 1.0263, 1.0256, 1.0262, 1.0244, 1.0225, 1.0235, 1.0254, 1.0256, 1.0252, 1.0262, 1.0259, 1.0254, 1.0254, 1.0254, 1.0242, 1.0246, 1.0262, 1.0253, 1.0254, 1.0254, 1.0254, 1.0258, 1.0259, 1.0254]\n",
      "  time use: 192.82986116409302\n",
      "44 evolution\n",
      "    survival: [6, 8, 22, 15, 16]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0271\n",
      "  Pop Fitness: [1.0259, 1.0258, 1.0251, 1.0264, 1.026, 1.0263, 1.0263, 1.0256, 1.0262, 1.0216, 1.0261, 1.0237, 1.0263, 1.0243, 1.0264, 1.0262, 1.0259, 1.0262, 1.0258, 1.0212, 1.026, 1.026, 1.0262, 1.0257, 1.0261, 1.025, 1.0271, 1.0263, 1.0257, 1.0263]\n",
      "  time use: 193.18484234809875\n",
      "45 evolution\n",
      "    survival: [26, 14, 3, 5, 6]\n",
      "    obsolete: [0, 1, 2, 4, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29]\n",
      "  Best Fitness: 1.0276\n",
      "  Pop Fitness: [1.0264, 1.0264, 1.0269, 1.0264, 1.0262, 1.0263, 1.0263, 1.0265, 1.0258, 1.0263, 1.0243, 1.0264, 1.023, 1.0266, 1.0264, 1.0257, 1.0267, 1.0259, 1.0276, 1.0254, 1.0262, 1.0272, 1.0271, 1.0253, 1.0269, 1.0257, 1.0271, 1.0261, 1.027, 1.0263]\n",
      "  time use: 193.96501421928406\n",
      "46 evolution\n",
      "    survival: [18, 21, 26, 22, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 1.0279\n",
      "  Pop Fitness: [1.0265, 1.0275, 1.0272, 1.0269, 1.0199, 1.0279, 1.0268, 1.0261, 1.0267, 1.0276, 1.0265, 1.0267, 1.0266, 1.0274, 1.0269, 1.0268, 1.0267, 1.0271, 1.0276, 1.0256, 1.0066, 1.0272, 1.0271, 1.0264, 1.0277, 1.0271, 1.0271, 1.0268, 1.027, 1.0264]\n",
      "  time use: 192.94054198265076\n",
      "47 evolution\n",
      "    survival: [5, 24, 9, 18, 1]\n",
      "    obsolete: [0, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0283\n",
      "  Pop Fitness: [1.0272, 1.0275, 1.0254, 1.0273, 1.0271, 1.0279, 1.0277, 1.0276, 1.0276, 1.0276, 1.0271, 1.0276, 1.0274, 1.027, 1.0269, 1.0268, 1.0252, 1.0269, 1.0276, 0.9496, 1.0274, 1.0265, 1.0279, 1.0269, 1.0277, 1.0279, 1.0269, 1.0283, 1.0276, 1.0276]\n",
      "  time use: 199.84888005256653\n",
      "48 evolution\n",
      "    survival: [27, 5, 22, 25, 6]\n",
      "    obsolete: [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 28, 29]\n",
      "  Best Fitness: 1.0291\n",
      "  Pop Fitness: [1.0276, 1.0276, 1.0274, 1.0277, 1.0261, 1.0279, 1.0277, 1.0272, 0.9718, 1.0273, 1.0251, 1.0272, 1.0254, 1.025, 1.0278, 1.0276, 1.0282, 1.028, 1.0274, 1.0281, 1.0279, 1.0278, 1.0279, 1.028, 1.0275, 1.0279, 1.0291, 1.0283, 1.0266, 1.0263]\n",
      "  time use: 198.12906551361084\n",
      "49 evolution\n",
      "    survival: [26, 27, 16, 19, 17]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 20, 21, 22, 23, 24, 25, 28, 29]\n",
      "  Best Fitness: 1.0291\n",
      "  Pop Fitness: [1.0287, 1.0286, 1.0271, 1.0271, 1.028, 1.0283, 1.0284, 1.0281, 1.028, 1.028, 1.029, 1.0281, 1.0262, 1.028, 1.0163, 1.0253, 1.0282, 1.028, 1.0258, 1.0281, 1.0275, 1.0283, 1.028, 1.0285, 1.0277, 1.0288, 1.0291, 1.0283, 1.0272, 1.0282]\n",
      "  time use: 193.9645824432373\n",
      "50 evolution\n",
      "    survival: [26, 10, 25, 0, 1]\n",
      "    obsolete: [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0292\n",
      "  Pop Fitness: [1.0287, 1.0286, 1.0266, 1.0292, 1.0276, 1.0281, 1.0288, 1.0285, 1.0292, 1.0285, 1.029, 1.0275, 1.0289, 1.0285, 1.0274, 1.0247, 1.0272, 1.028, 1.0126, 1.0292, 1.0292, 1.0286, 1.0281, 1.0288, 1.0281, 1.0288, 1.0291, 1.0286, 1.0292, 1.0282]\n",
      "  time use: 195.95598149299622\n",
      "51 evolution\n",
      "    survival: [3, 19, 28, 8, 20]\n",
      "    obsolete: [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0297\n",
      "  Pop Fitness: [1.0281, 1.0288, 1.0284, 1.0292, 1.0291, 1.0288, 1.028, 1.0283, 1.0292, 1.0289, 1.0284, 1.0292, 1.029, 1.0297, 1.0292, 1.028, 1.0257, 1.0288, 1.0291, 1.0292, 1.0292, 1.0059, 1.0252, 1.0292, 1.0279, 1.0295, 1.0282, 1.0291, 1.0292, 1.0283]\n",
      "  time use: 193.09165620803833\n",
      "52 evolution\n",
      "    survival: [13, 25, 3, 11, 14]\n",
      "    obsolete: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0300\n",
      "  Pop Fitness: [1.0273, 1.0283, 1.0295, 1.0292, 1.0295, 1.0284, 0.9976, 1.0294, 1.0291, 1.0295, 1.0279, 1.0292, 1.027, 1.0297, 1.0292, 1.0285, 1.0299, 1.0281, 1.03, 1.029, 1.0298, 1.029, 1.0295, 1.0292, 1.0291, 1.0295, 1.0292, 1.0294, 1.0295, 1.0288]\n",
      "  time use: 197.3989496231079\n",
      "53 evolution\n",
      "    survival: [18, 16, 20, 13, 25]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 19, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0308\n",
      "  Pop Fitness: [1.0291, 1.0291, 1.0299, 1.0182, 1.0293, 1.0297, 1.028, 1.0283, 1.0299, 1.0275, 1.0307, 1.0291, 1.0284, 1.0297, 1.0296, 1.0298, 1.0299, 1.0286, 1.03, 1.0308, 1.0298, 1.0293, 1.0285, 1.0273, 1.0295, 1.0295, 1.03, 1.0295, 1.0293, 1.0296]\n",
      "  time use: 199.90694761276245\n",
      "54 evolution\n",
      "    survival: [19, 10, 26, 18, 2]\n",
      "    obsolete: [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 27, 28, 29]\n",
      "  Best Fitness: 1.0315\n",
      "  Pop Fitness: [1.0295, 1.0278, 1.0299, 1.0266, 1.0223, 1.0212, 1.0297, 1.0299, 1.0243, 1.0276, 1.0307, 1.0306, 1.0288, 1.0292, 1.0293, 1.0295, 1.0299, 1.0307, 1.03, 1.0308, 1.0304, 1.0301, 1.0307, 1.0305, 1.0315, 1.0297, 1.03, 1.0293, 1.03, 1.0288]\n",
      "  time use: 197.85873794555664\n",
      "55 evolution\n",
      "    survival: [24, 19, 17, 10, 22]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 20, 21, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0321\n",
      "  Pop Fitness: [1.0305, 1.0301, 1.0299, 1.0303, 0.9452, 1.0301, 1.0317, 1.025, 1.0309, 1.0299, 1.0307, 1.0293, 1.0305, 1.0301, 1.0304, 1.0281, 1.0319, 1.0307, 1.0301, 1.0308, 1.0313, 1.0307, 1.0307, 1.0073, 1.0315, 1.0315, 1.0315, 1.0321, 1.0319, 1.032]\n",
      "  time use: 193.12771081924438\n",
      "56 evolution\n",
      "    survival: [27, 29, 16, 28, 6]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "  Best Fitness: 1.0323\n",
      "  Pop Fitness: [1.0295, 1.0313, 1.0316, 1.0312, 1.031, 1.0311, 1.0317, 1.0053, 1.0317, 1.0316, 1.0316, 1.0319, 1.0224, 1.03, 1.0252, 1.0319, 1.0319, 1.0303, 1.0321, 1.0321, 1.0317, 1.0169, 1.0323, 1.032, 1.0305, 1.0318, 1.0314, 1.0321, 1.0319, 1.032]\n",
      "  time use: 193.36749958992004\n",
      "57 evolution\n",
      "    survival: [22, 19, 27, 18, 23]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 21, 24, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0326\n",
      "  Pop Fitness: [1.0324, 1.0323, 1.0302, 1.0284, 1.0292, 1.032, 1.0319, 1.0274, 1.0298, 1.0313, 1.0318, 1.0312, 1.0316, 1.0319, 1.0313, 1.0315, 1.0325, 1.0321, 1.0321, 1.0321, 1.0312, 1.0319, 1.0323, 1.032, 1.0321, 1.0319, 1.0326, 1.0321, 1.0321, 1.0318]\n",
      "  time use: 192.7125542163849\n",
      "58 evolution\n",
      "    survival: [26, 16, 0, 1, 22]\n",
      "    obsolete: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 23, 24, 25, 27, 28, 29]\n",
      "  Best Fitness: 1.0328\n",
      "  Pop Fitness: [1.0324, 1.0323, 1.0324, 1.0308, 1.0281, 1.0311, 1.0317, 1.0326, 1.0291, 1.0322, 1.0292, 1.0324, 1.0315, 1.0324, 1.0319, 1.0299, 1.0325, 1.0323, 1.0323, 1.03, 1.0328, 1.0317, 1.0323, 1.0328, 1.0321, 1.0326, 1.0326, 1.0323, 1.0324, 1.0325]\n",
      "  time use: 190.41117882728577\n",
      "59 evolution\n",
      "    survival: [23, 20, 25, 26, 7]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0328\n",
      "  Pop Fitness: [1.0317, 1.032, 1.018, 1.0319, 1.0305, 1.0311, 1.0327, 1.0326, 1.0317, 1.0291, 1.0328, 1.0302, 1.0325, 1.0322, 1.0323, 1.031, 1.0318, 1.0324, 1.0328, 1.0326, 1.0328, 1.0323, 1.0324, 1.0328, 1.0325, 1.0326, 1.0326, 1.0322, 1.0326, 1.0328]\n",
      "  time use: 196.2102472782135\n",
      "60 evolution\n",
      "    survival: [10, 23, 29, 18, 20]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0331\n",
      "  Pop Fitness: [1.0252, 0.9922, 1.0291, 1.0329, 1.0281, 1.0328, 1.0324, 1.033, 1.0327, 1.0322, 1.0328, 1.032, 1.0228, 1.0312, 1.0307, 1.0321, 1.0331, 1.0312, 1.0328, 1.0326, 1.0328, 1.0319, 1.0319, 1.0328, 1.0329, 1.0326, 1.0321, 1.0322, 1.0328, 1.0328]\n",
      "  time use: 199.45648288726807\n",
      "61 evolution\n",
      "    survival: [16, 7, 24, 3, 5]\n",
      "    obsolete: [0, 1, 2, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0333\n",
      "  Pop Fitness: [1.0326, 1.0323, 1.0314, 1.0329, 1.0288, 1.0328, 1.0328, 1.033, 1.0326, 1.0323, 1.0333, 1.0331, 1.0321, 1.033, 1.0328, 1.0322, 1.0331, 1.0324, 1.0324, 1.0321, 1.0309, 1.03, 1.0326, 1.0323, 1.0329, 1.033, 1.0329, 1.0328, 1.0325, 1.0325]\n",
      "  time use: 194.8451919555664\n",
      "62 evolution\n",
      "    survival: [10, 11, 16, 7, 13]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 8, 9, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0333\n",
      "  Pop Fitness: [1.0301, 1.0331, 1.0324, 1.0326, 1.0325, 1.0321, 1.0325, 1.033, 1.0325, 1.0316, 1.0333, 1.0331, 1.0314, 1.033, 1.0286, 1.0323, 1.0331, 1.0324, 1.0329, 1.0329, 1.0318, 1.0323, 1.0322, 1.0327, 1.0331, 1.0328, 1.0333, 1.0324, 1.0331, 1.0328]\n",
      "  time use: 193.51213335990906\n",
      "63 evolution\n",
      "    survival: [10, 26, 1, 28, 11]\n",
      "    obsolete: [0, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 1.0338\n",
      "  Pop Fitness: [1.0305, 1.0331, 1.0329, 1.033, 1.0257, 1.0182, 1.0334, 1.0328, 1.0335, 1.0317, 1.0333, 1.0331, 1.0314, 1.0312, 1.0332, 1.0328, 1.0333, 1.0327, 1.0245, 1.0326, 1.031, 1.033, 1.0325, 1.0328, 1.0331, 1.033, 1.0333, 1.0325, 1.0331, 1.0338]\n",
      "  time use: 195.7510588169098\n",
      "64 evolution\n",
      "    survival: [29, 8, 6, 16, 10]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 9, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0339\n",
      "  Pop Fitness: [1.0331, 1.0151, 1.0328, 1.0333, 1.0324, 1.0335, 1.0334, 1.0329, 1.0335, 1.0332, 1.0333, 1.0334, 1.0318, 1.0316, 1.0331, 1.0328, 1.0333, 1.0255, 1.0329, 1.0324, 1.0339, 1.0311, 1.0321, 1.0336, 1.0331, 1.0327, 1.0334, 1.0331, 1.0332, 1.0338]\n",
      "  time use: 195.81038355827332\n",
      "65 evolution\n",
      "    survival: [20, 29, 23, 5, 8]\n",
      "    obsolete: [0, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0340\n",
      "  Pop Fitness: [1.0332, 1.0335, 1.0335, 1.0321, 1.0331, 1.0335, 1.0338, 1.0226, 1.0335, 1.0335, 1.0304, 1.0309, 1.0335, 1.034, 1.0332, 1.0335, 1.0338, 1.0312, 1.033, 1.0305, 1.0339, 1.0331, 1.0328, 1.0336, 1.0333, 1.0335, 1.0336, 1.0334, 1.0332, 1.0338]\n",
      "  time use: 195.39255118370056\n",
      "66 evolution\n",
      "    survival: [13, 20, 16, 6, 29]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0342\n",
      "  Pop Fitness: [1.0329, 1.0331, 1.0329, 1.0333, 1.0331, 1.0337, 1.0338, 1.034, 1.0322, 1.0321, 1.0299, 1.0315, 1.0331, 1.034, 1.0333, 1.0342, 1.0338, 1.0325, 1.0324, 1.0327, 1.0339, 1.0332, 1.0324, 1.033, 1.0338, 1.0338, 1.0336, 1.0336, 1.034, 1.0338]\n",
      "  time use: 196.51054096221924\n",
      "67 evolution\n",
      "    survival: [15, 7, 13, 28, 20]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0342\n",
      "  Pop Fitness: [1.0308, 1.0338, 1.0332, 1.0328, 1.0329, 1.0325, 1.0292, 1.034, 1.0335, 1.0336, 1.0336, 1.031, 1.0321, 1.034, 1.0338, 1.0342, 1.0316, 1.0337, 1.0049, 1.0338, 1.0339, 1.0334, 1.0311, 1.0338, 1.0333, 1.0341, 1.0336, 1.0324, 1.034, 1.0332]\n",
      "  time use: 195.763445854187\n",
      "68 evolution\n",
      "    survival: [15, 25, 7, 13, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 29]\n",
      "  Best Fitness: 1.0342\n",
      "  Pop Fitness: [1.0304, 1.0335, 1.0341, 1.034, 1.0336, 1.0342, 1.0324, 1.034, 1.0339, 0.8669, 1.0335, 1.0326, 1.0337, 1.034, 1.0329, 1.0342, 1.0274, 1.0335, 1.0325, 1.0339, 1.0326, 1.0338, 1.0336, 1.0336, 1.0337, 1.0341, 1.0339, 1.034, 1.034, 1.0339]\n",
      "  time use: 195.393235206604\n",
      "69 evolution\n",
      "    survival: [5, 15, 2, 25, 3]\n",
      "    obsolete: [0, 1, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0344\n",
      "  Pop Fitness: [1.0329, 1.034, 1.0341, 1.034, 1.0335, 1.0342, 1.0344, 1.0317, 1.0312, 1.0336, 1.0328, 1.0327, 1.0296, 1.0326, 1.0333, 1.0342, 1.0339, 0.9644, 1.0313, 1.0338, 1.0338, 1.0336, 1.0336, 1.0331, 1.0339, 1.0341, 1.0338, 1.0336, 1.0336, 1.0334]\n",
      "  time use: 195.45814442634583\n",
      "70 evolution\n",
      "    survival: [6, 5, 15, 2, 25]\n",
      "    obsolete: [0, 1, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0344\n",
      "  Pop Fitness: [1.0321, 1.0326, 1.0341, 1.0328, 1.0338, 1.0342, 1.0344, 1.0336, 1.0324, 1.0335, 1.0341, 1.0319, 1.0329, 1.0323, 1.0322, 1.0342, 1.034, 1.0331, 1.0333, 1.0295, 1.034, 1.0312, 1.0313, 1.0317, 1.0334, 1.0341, 1.0339, 1.0342, 1.0342, 1.0336]\n",
      "  time use: 198.26776266098022\n",
      "71 evolution\n",
      "    survival: [6, 5, 15, 27, 28]\n",
      "    obsolete: [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29]\n",
      "  Best Fitness: 1.0344\n",
      "  Pop Fitness: [1.0335, 1.0342, 1.0328, 1.032, 1.0283, 1.0342, 1.0344, 1.0343, 1.0317, 1.0342, 1.0335, 1.0329, 1.0271, 1.0329, 1.0324, 1.0342, 1.0326, 1.0331, 1.0339, 1.0338, 1.0293, 1.0321, 1.0342, 1.0342, 1.0342, 1.0334, 1.0342, 1.0342, 1.0342, 1.0342]\n",
      "  time use: 196.08213448524475\n",
      "72 evolution\n",
      "    survival: [6, 7, 1, 22, 29]\n",
      "    obsolete: [0, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28]\n",
      "  Best Fitness: 1.0345\n",
      "  Pop Fitness: [1.0343, 1.0342, 1.0302, 1.0338, 1.0337, 1.0339, 1.0344, 1.0343, 1.0324, 1.0336, 1.0316, 1.0343, 1.0332, 1.0336, 1.0319, 1.0311, 1.0342, 1.034, 1.0335, 1.0319, 1.0342, 1.0345, 1.0342, 1.0341, 1.0338, 1.034, 1.034, 1.0342, 1.0342, 1.0342]\n",
      "  time use: 194.78653979301453\n",
      "73 evolution\n",
      "    survival: [21, 6, 0, 7, 11]\n",
      "    obsolete: [1, 2, 3, 4, 5, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0345\n",
      "  Pop Fitness: [1.0343, 1.0343, 1.0322, 1.0329, 1.0337, 1.0309, 1.0344, 1.0343, 1.0343, 1.0053, 1.0322, 1.0343, 1.0338, 1.0342, 1.0332, 1.022, 1.0343, 1.0324, 1.034, 1.0329, 1.0323, 1.0345, 1.0326, 1.0264, 1.0342, 1.0342, 1.0333, 1.0338, 1.0343, 1.0334]\n",
      "  time use: 195.446031332016\n",
      "74 evolution\n",
      "    survival: [21, 6, 0, 1, 7]\n",
      "    obsolete: [2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0345\n",
      "  Pop Fitness: [1.0343, 1.0343, 1.0343, 1.0336, 1.0328, 1.0343, 1.0344, 1.0343, 1.0335, 1.0323, 1.0316, 1.0335, 1.0338, 1.033, 1.0323, 1.0332, 1.0335, 1.0343, 1.0311, 1.0337, 1.0338, 1.0345, 1.0302, 1.0331, 1.0301, 1.0336, 1.0335, 1.0338, 1.0339, 1.0339]\n",
      "  time use: 194.41442584991455\n",
      "75 evolution\n",
      "    survival: [21, 6, 0, 1, 2]\n",
      "    obsolete: [3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0343, 1.0343, 1.0343, 1.0331, 1.0333, 1.0318, 1.0344, 1.0336, 1.032, 1.0319, 1.0333, 1.0332, 1.0327, 1.0333, 1.0322, 1.0344, 1.0343, 0.9217, 1.0341, 1.0345, 1.0348, 1.0345, 1.0314, 1.0335, 1.0337, 1.0339, 1.0341, 1.0339, 1.0335, 1.0334]\n",
      "  time use: 197.1510682106018\n",
      "76 evolution\n",
      "    survival: [20, 19, 21, 6, 15]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.032, 1.0328, 1.0337, 1.0317, 1.0338, 1.0327, 1.0344, 1.0333, 1.0345, 1.0323, 1.0344, 1.0328, 1.0342, 1.0309, 1.0339, 1.0344, 1.0327, 1.0336, 1.034, 1.0345, 1.0348, 1.0345, 1.0342, 1.0331, 1.0345, 1.0344, 1.0343, 1.0345, 1.0344, 1.0342]\n",
      "  time use: 198.4813084602356\n",
      "77 evolution\n",
      "    survival: [20, 19, 24, 8, 21]\n",
      "    obsolete: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0283, 1.034, 1.0283, 1.034, 1.0347, 1.0324, 1.032, 1.034, 1.0345, 1.0335, 1.034, 1.0326, 1.0338, 1.0309, 1.0338, 1.0339, 1.034, 1.0335, 1.0336, 1.0345, 1.0348, 1.0345, 1.0328, 1.0284, 1.0345, 1.0344, 1.0342, 1.0345, 1.0348, 1.0343]\n",
      "  time use: 199.63839507102966\n",
      "78 evolution\n",
      "    survival: [20, 28, 4, 19, 24]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.033, 1.034, 1.0337, 1.0338, 1.0347, 1.034, 1.0345, 1.0326, 1.0306, 0.9988, 1.0302, 1.0329, 1.0338, 1.0334, 1.0339, 1.0342, 1.0333, 1.0316, 1.034, 1.0345, 1.0348, 1.0344, 1.034, 1.0344, 1.0345, 1.0343, 1.0345, 1.0343, 1.0348, 1.034]\n",
      "  time use: 194.61950588226318\n",
      "79 evolution\n",
      "    survival: [20, 28, 4, 19, 24]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0345, 1.0347, 1.0345, 1.0306, 1.0347, 1.0295, 1.0328, 1.0324, 1.0324, 1.0329, 1.0336, 1.0335, 1.0331, 1.033, 1.0323, 1.0338, 1.0336, 1.0347, 0.7995, 1.0345, 1.0348, 1.0328, 1.0307, 1.0345, 1.0345, 1.0347, 1.0343, 1.0332, 1.0348, 1.0342]\n",
      "  time use: 196.23440432548523\n",
      "80 evolution\n",
      "    survival: [20, 28, 25, 1, 4]\n",
      "    obsolete: [0, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26, 27, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0313, 1.0347, 1.0347, 1.0301, 1.0347, 1.0342, 1.0317, 0.9252, 1.0346, 1.0345, 1.0338, 1.0339, 1.0342, 1.0331, 1.0333, 1.0328, 1.0324, 1.0318, 1.0342, 1.0335, 1.0348, 1.0345, 1.034, 1.0343, 1.0345, 1.0347, 1.0348, 1.0336, 1.0348, 1.0345]\n",
      "  time use: 197.25561213493347\n",
      "81 evolution\n",
      "    survival: [20, 26, 28, 2, 25]\n",
      "    obsolete: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 27, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0335, 1.034, 1.0347, 1.0342, 1.0342, 1.0283, 1.0342, 1.0306, 1.0336, 1.0345, 1.0313, 1.0335, 1.0194, 1.0338, 1.0348, 1.0326, 1.0342, 1.0322, 1.0343, 1.0347, 1.0348, 1.0338, 1.0341, 1.0347, 1.0344, 1.0347, 1.0348, 1.0343, 1.0348, 1.0345]\n",
      "  time use: 195.4092664718628\n",
      "82 evolution\n",
      "    survival: [14, 20, 26, 28, 2]\n",
      "    obsolete: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0331, 1.0336, 1.0347, 1.0338, 1.0348, 1.0256, 1.0346, 1.0329, 1.0302, 1.033, 1.0298, 1.0295, 1.0346, 1.0347, 1.0348, 1.0333, 1.0336, 1.0342, 1.0345, 1.032, 1.0348, 1.0345, 1.034, 1.0346, 1.0343, 1.0342, 1.0348, 1.0346, 1.0348, 1.0344]\n",
      "  time use: 195.41544842720032\n",
      "83 evolution\n",
      "    survival: [4, 14, 20, 26, 28]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 27, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0324, 1.0338, 1.0344, 1.0346, 1.0348, 1.0334, 1.032, 1.0333, 1.0324, 1.0314, 1.0324, 1.0326, 1.0345, 1.0332, 1.0348, 1.0338, 1.0347, 1.0324, 1.005, 1.0346, 1.0348, 1.0343, 1.0318, 1.034, 1.0345, 1.0348, 1.0348, 1.0346, 1.0348, 1.034]\n",
      "  time use: 195.18145871162415\n",
      "84 evolution\n",
      "    survival: [4, 14, 20, 25, 26]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0348\n",
      "  Pop Fitness: [1.0326, 1.0347, 1.034, 1.029, 1.0348, 1.0344, 1.0334, 1.0345, 1.0319, 1.0302, 1.0302, 1.0324, 1.034, 1.0335, 1.0348, 1.0332, 1.033, 1.0317, 1.0342, 1.0337, 1.0348, 1.0338, 1.0332, 1.0342, 1.0345, 1.0348, 1.0348, 1.0342, 1.0346, 1.0348]\n",
      "  time use: 195.2349090576172\n",
      "85 evolution\n",
      "    survival: [4, 14, 20, 25, 26]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0349\n",
      "  Pop Fitness: [1.0345, 1.033, 1.0344, 1.0343, 1.0348, 1.0337, 1.0342, 1.0349, 1.0346, 1.0316, 1.0345, 1.0227, 1.0309, 1.0339, 1.0348, 1.0344, 1.0328, 1.034, 1.0335, 1.0238, 1.0348, 1.0336, 1.0187, 1.0346, 1.0342, 1.0348, 1.0348, 1.0346, 1.0348, 1.0341]\n",
      "  time use: 194.5994713306427\n",
      "86 evolution\n",
      "    survival: [7, 4, 14, 20, 25]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0349\n",
      "  Pop Fitness: [1.0062, 1.0318, 1.0345, 1.0331, 1.0348, 1.0326, 1.0323, 1.0349, 1.0336, 1.032, 1.0338, 1.0319, 1.0278, 1.0336, 1.0348, 1.0328, 1.0335, 1.0333, 1.0342, 1.0331, 1.0348, 1.0329, 1.0324, 1.0336, 1.0343, 1.0348, 1.0345, 1.0344, 1.0343, 1.0342]\n",
      "  time use: 195.61555576324463\n",
      "87 evolution\n",
      "    survival: [7, 4, 14, 20, 25]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0349\n",
      "  Pop Fitness: [1.0311, 1.034, 1.0347, 1.0316, 1.0348, 1.0308, 1.034, 1.0349, 1.031, 1.0345, 1.0339, 1.0319, 1.034, 1.0287, 1.0348, 1.0346, 1.0319, 1.0317, 1.0295, 1.0336, 1.0348, 1.0321, 1.0324, 1.0343, 1.0348, 1.0348, 1.0348, 1.0342, 1.0344, 1.0343]\n",
      "  time use: 195.90443968772888\n",
      "88 evolution\n",
      "    survival: [7, 24, 4, 14, 20]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0350\n",
      "  Pop Fitness: [1.0348, 1.0328, 1.0348, 1.0347, 1.0348, 1.0339, 1.0328, 1.0349, 1.0338, 1.0348, 1.0331, 1.0139, 1.0323, 1.0336, 1.0348, 1.035, 1.0341, 1.0347, 1.0334, 1.0344, 1.0348, 1.0294, 1.0338, 1.0333, 1.0348, 1.0347, 1.0342, 1.0343, 1.0348, 1.0345]\n",
      "  time use: 194.33651733398438\n",
      "89 evolution\n",
      "    survival: [15, 7, 2, 9, 24]\n",
      "    obsolete: [0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0350\n",
      "  Pop Fitness: [1.0337, 1.0336, 1.0348, 1.0216, 1.035, 1.0321, 1.0305, 1.0349, 1.034, 1.0348, 1.0317, 1.0306, 1.0335, 1.0264, 1.0344, 1.035, 1.0347, 1.0348, 1.03, 1.0241, 1.0338, 1.0343, 1.033, 1.0329, 1.0348, 1.0348, 1.0347, 1.0348, 1.0349, 1.0346]\n",
      "  time use: 196.09016227722168\n",
      "90 evolution\n",
      "    survival: [4, 15, 7, 28, 2]\n",
      "    obsolete: [0, 1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0350\n",
      "  Pop Fitness: [1.0343, 1.0326, 1.0348, 1.0141, 1.035, 1.0345, 1.0343, 1.0349, 1.0315, 0.999, 1.0301, 1.0347, 1.0321, 1.0315, 1.0348, 1.035, 1.0348, 1.0341, 1.0333, 1.0344, 1.0342, 1.0347, 1.0345, 1.0342, 1.0347, 1.0347, 1.0347, 1.0348, 1.0349, 1.0342]\n",
      "  time use: 194.2942612171173\n",
      "91 evolution\n",
      "    survival: [4, 15, 7, 28, 2]\n",
      "    obsolete: [0, 1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29]\n",
      "  Best Fitness: 1.0350\n",
      "  Pop Fitness: [1.0339, 1.0326, 1.0348, 1.0348, 1.035, 1.0337, 1.0316, 1.0349, 1.0305, 1.033, 1.033, 1.0336, 1.0313, 1.0299, 1.0329, 1.035, 1.0091, 1.0348, 1.0343, 0.9925, 1.0314, 1.034, 1.0319, 1.0345, 1.0347, 1.0348, 1.0347, 1.0349, 1.0349, 1.0345]\n",
      "  time use: 193.62506461143494\n",
      "92 evolution\n",
      "    survival: [4, 15, 7, 27, 28]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29]\n",
      "  Best Fitness: 1.0350\n",
      "  Pop Fitness: [1.0348, 1.0342, 1.033, 1.0291, 1.035, 1.0348, 1.0293, 1.0349, 1.0335, 1.0347, 1.0332, 1.0294, 1.0342, 1.0303, 1.0323, 1.035, 1.0348, 1.0292, 1.0333, 1.0346, 1.0325, 1.0305, 1.03, 1.0347, 1.0348, 1.0349, 1.035, 1.0349, 1.0349, 1.0346]\n",
      "  time use: 194.90965127944946\n",
      "93 evolution\n",
      "    survival: [4, 15, 26, 7, 25]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0350\n",
      "  Pop Fitness: [1.0347, 1.0283, 1.0336, 1.033, 1.035, 1.0337, 1.0342, 1.0349, 1.0324, 1.0325, 1.0328, 1.0347, 1.0344, 1.0344, 1.0348, 1.035, 1.034, 1.0327, 1.0316, 1.0215, 1.034, 1.0337, 1.0341, 1.0346, 1.0343, 1.0349, 1.035, 1.0346, 1.0349, 1.0347]\n",
      "  time use: 195.20389032363892\n",
      "94 evolution\n",
      "    survival: [4, 15, 26, 7, 25]\n",
      "    obsolete: [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29]\n",
      "  Best Fitness: 1.0352\n",
      "  Pop Fitness: [1.0295, 1.0269, 1.0339, 1.035, 1.035, 1.0352, 1.0342, 1.0349, 0.961, 1.0319, 1.0304, 1.0312, 1.0347, 1.0346, 1.0348, 1.035, 1.0343, 1.0345, 1.0338, 1.0331, 1.0329, 1.0341, 1.0336, 1.0348, 1.0352, 1.0349, 1.035, 1.0346, 1.0348, 1.035]\n",
      "  time use: 193.8929078578949\n",
      "95 evolution\n",
      "    survival: [5, 24, 3, 4, 15]\n",
      "    obsolete: [0, 1, 2, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0352\n",
      "  Pop Fitness: [1.0341, 1.0336, 1.033, 1.035, 1.035, 1.0352, 1.033, 1.0329, 1.0334, 1.034, 1.0335, 1.0337, 1.0352, 1.0342, 1.0348, 1.035, 1.0295, 1.0345, 1.034, 1.0343, 1.0336, 1.0126, 1.0347, 1.0348, 1.0352, 1.035, 1.0348, 1.0345, 1.0346, 1.0346]\n",
      "  time use: 197.64142370224\n",
      "96 evolution\n",
      "    survival: [12, 5, 24, 3, 4]\n",
      "    obsolete: [0, 1, 2, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29]\n",
      "  Best Fitness: 1.0352\n",
      "  Pop Fitness: [1.0342, 1.033, 1.0347, 1.035, 1.035, 1.0352, 1.0339, 1.0131, 1.0338, 0.9869, 1.0333, 1.0334, 1.0352, 1.0349, 1.0348, 1.0338, 1.0348, 1.0341, 1.0338, 1.0344, 1.0327, 1.033, 1.035, 1.0352, 1.0352, 1.0348, 1.0345, 1.0352, 1.0347, 1.0345]\n",
      "  time use: 196.21313428878784\n",
      "97 evolution\n",
      "    survival: [12, 5, 23, 24, 27]\n",
      "    obsolete: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0352\n",
      "  Pop Fitness: [1.0339, 1.0351, 1.035, 1.0325, 1.0309, 1.0352, 1.0324, 1.0338, 1.0345, 1.0332, 1.0335, 1.0345, 1.0352, 1.0351, 1.0347, 1.0309, 1.0326, 1.035, 1.035, 1.0323, 1.0336, 1.03, 1.0348, 1.0352, 1.0352, 1.035, 1.0349, 1.0352, 1.0347, 1.0348]\n",
      "  time use: 195.76920866966248\n",
      "98 evolution\n",
      "    survival: [12, 5, 23, 24, 27]\n",
      "    obsolete: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0352\n",
      "  Pop Fitness: [1.0339, 1.0346, 1.0337, 1.0349, 1.0318, 1.0352, 1.0305, 1.0343, 1.0331, 1.0333, 1.0348, 1.0333, 1.0352, 1.0289, 1.0306, 1.0347, 1.0347, 1.0324, 1.0283, 1.0339, 1.0335, 1.0326, 1.035, 1.0352, 1.0352, 1.0348, 1.0347, 1.0352, 1.0347, 1.0347]\n",
      "  time use: 195.55277514457703\n",
      "99 evolution\n",
      "    survival: [12, 5, 23, 24, 27]\n",
      "    obsolete: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0352\n",
      "  Pop Fitness: [1.0036, 1.0337, 1.035, 1.0343, 1.0319, 1.0352, 1.0338, 1.0339, 1.0307, 1.034, 1.0347, 1.0348, 1.0352, 1.0347, 1.0348, 1.0343, 1.0351, 1.0338, 1.0342, 1.0331, 1.0318, 0.9846, 1.0348, 1.0352, 1.0352, 1.0347, 1.0349, 1.0352, 1.0342, 1.0345]\n",
      "  time use: 195.74582815170288\n",
      "100 evolution\n",
      "    survival: [12, 5, 23, 24, 27]\n",
      "    obsolete: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 28, 29]\n",
      "  Best Fitness: 1.0353\n",
      "  Pop Fitness: [1.0329, 1.0342, 1.0327, 1.035, 1.0311, 1.0352, 1.0295, 1.0342, 1.0347, 1.0312, 1.035, 1.0233, 1.0352, 1.0346, 1.0324, 1.0243, 1.0343, 1.0218, 1.0316, 1.0334, 1.0266, 1.0339, 1.0348, 1.0352, 1.0352, 1.0353, 1.0352, 1.0352, 1.0348, 1.0345]\n",
      "  time use: 195.94858813285828\n",
      "Drop Set: [15, 16, 33, 48, 53, 68, 70, 87, 93, 126, 155, 161, 163, 170, 191, 198, 230, 239, 328, 336, 353, 400, 402, 423, 431, 471, 492, 494, 502, 522, 557, 567, 589, 596, 605, 606, 618, 622, 625, 628, 631, 661, 675, 688, 746, 768, 777, 780, 797, 808, 845, 881, 896, 908, 909, 919, 920, 929, 931, 935, 951, 986, 1034, 1047, 1049, 1058, 1079, 1134, 1150, 1222, 1285, 1299, 1328, 1334, 1375, 1384, 1394, 1406, 1420, 1444, 1461, 1478, 1481, 1498, 1579, 1591, 1614, 1620, 1628, 1690, 1733, 1812, 1814, 1825, 1836, 1852, 1858, 1871, 1875, 1881, 1905, 1916, 1929, 1978, 2008, 2036, 2037, 2053, 2056, 2066, 2068, 2151, 2163, 2180, 2184, 2196, 2202, 2213, 2225, 2240, 2261, 2264, 2282, 2288, 2300, 2303, 2304, 2306, 2313, 2322, 2327, 2340, 2345, 2347, 2394, 2409, 2437, 2464, 2496, 2509, 2522, 2530, 2544, 2579, 2587, 2620, 2626, 2632, 2668, 2670, 2679, 2680, 2689, 2695, 2715, 2720, 2722, 2724, 2727, 2745, 2775, 2795, 2797, 2804, 2845, 2855, 2856, 2873, 2907, 2908, 2942, 2948, 2960, 2989, 3018, 3035, 3052, 3059, 3084, 3126, 3150, 3153, 3156, 3161, 3167, 3172, 3194, 3198, 3220, 3250, 3290, 3312, 3324, 3340, 3369, 3375, 3381, 3431, 3437, 3492, 3493, 3521, 3556, 3563, 3571, 3572, 3577, 3579, 3589, 3639, 3649, 3664, 3676, 3702, 3709, 3710, 3728, 3759, 3774, 3780, 3782, 3787, 3789, 3791, 3830, 3849, 3856, 3880, 3899, 3900, 3906, 3912, 3934, 3942, 3947, 3953, 3957, 3972, 3981, 3993, 4006, 4022, 4039, 4103, 4119, 4122, 4130, 4161, 4178, 4187, 4192, 4195, 4205, 4208, 4209, 4222, 4224, 4268, 4272, 4299, 4327, 4330, 4374, 4379, 4386, 4402, 4442, 4457, 4462, 4464, 4487, 4491, 4493, 4497, 4521, 4523, 4524, 4529, 4538, 4559, 4606, 4607, 4622, 4632, 4643, 4660, 4662, 4681, 4683, 4687, 4727, 4764, 4768, 4806, 4807, 4812, 4815, 4823, 4836, 4845, 4856, 4870, 4896, 4925, 4974, 4977, 4989, 4994, 5037, 5054, 5069, 5085, 5088, 5097, 5099, 5114, 5144, 5160, 5174, 5197, 5203, 5237, 5240, 5251, 5258, 5281, 5337, 5375, 5413, 5416, 5444, 5466, 5482, 5485, 5487, 5493, 5512, 5517, 5542, 5564, 5634, 5682, 5705, 5754, 5756, 5761, 5792, 5807, 5816, 5845, 5880, 5911, 5912, 5922, 5941, 5942, 5944, 5956, 6004, 6027, 6034, 6038, 6048, 6054, 6076, 6083, 6087, 6149, 6156, 6161, 6176, 6182, 6225, 6234, 6262, 6281, 6303, 6313, 6360, 6371, 6384, 6389, 6408, 6428, 6445, 6472, 6474, 6490, 6494, 6561, 6599, 6623, 6631, 6640, 6644, 6649, 6664, 6669, 6690, 6709, 6732, 6736, 6741, 6767, 6778, 6794, 6804, 6821, 6834, 6843, 6850, 6865, 6866, 6869, 6884, 6902, 6915, 6923, 6939, 6969, 6971, 6980, 6996, 7074, 7075, 7093, 7136, 7138, 7141, 7151, 7165, 7183, 7194, 7197, 7199, 7205, 7222, 7236, 7247, 7274, 7278, 7294, 7302, 7313, 7325, 7326, 7336, 7340, 7344, 7368, 7376, 7401, 7432, 7446, 7459, 7482, 7484, 7493, 7503, 7558, 7575, 7603, 7612, 7622, 7631, 7641, 7651, 7671, 7677, 7692, 7719, 7729, 7734, 7738, 7747, 7750, 7804, 7806, 7815, 7818, 7828, 7836, 7850, 7895, 7916, 7929, 7959, 7971, 7974, 7988, 7991, 7997, 8053, 8058, 8065, 8075, 8078, 8080, 8088, 8097, 8127, 8130, 8147, 8157, 8167, 8186, 8229, 8274, 8275, 8295, 8301, 8321, 8323, 8333, 8337, 8372, 8375, 8389, 8427, 8433, 8437, 8449, 8450, 8454, 8467, 8477, 8513, 8531, 8532, 8536, 8544, 8567, 8577, 8594, 8628, 8642, 8648, 8652, 8659, 8662, 8682, 8683, 8686, 8688, 8713, 8747, 8751, 8759, 8791, 8798, 8841, 8859, 8867, 8870, 8877, 8880, 8924, 8960, 8966, 8995, 9005, 9017, 9031, 9110, 9117, 9118, 9124, 9133, 9139, 9140, 9150, 9165, 9168, 9173, 9192, 9235, 9269, 9271, 9280, 9287, 9310, 9340, 9379, 9394, 9401, 9411, 9419, 9429, 9478, 9480, 9494, 9519, 9553, 9563, 9574, 9594, 9609, 9644, 9661, 9701, 9718, 9745, 9755, 9760, 9771, 9778, 9783, 9842, 9859, 9873, 9875, 9916, 9948, 9961, 10011, 10017, 10060, 10094, 10110, 10124, 10129, 10134, 10164, 10193, 10195, 10220, 10233, 10242, 10255, 10257, 10303, 10348, 10362, 10368, 10387, 10393, 10409, 10454, 10481, 10491, 10536, 10570, 10580, 10641, 10655, 10679, 10711, 10728, 10744, 10777, 10825, 10838, 10883, 10897, 10898, 10912, 10913, 10929, 10933, 10936, 10945, 10979, 11003, 11026, 11055, 11057, 11058, 11077, 11089, 11111, 11127, 11131, 11179, 11211, 11215, 11232, 11259, 11261, 11267, 11294, 11324, 11346, 11394, 11398, 11402, 11411, 11437, 11465, 11470, 11551, 11592, 11623, 11632, 11642, 11670, 11681, 11695, 11758, 11764, 11769, 11773, 11779, 11780, 11784, 11792, 11798, 11811, 11849, 11850, 11906, 11915, 11923, 11926, 11932, 11964, 11966, 11969, 12018, 12037, 12044, 12047, 12051, 12056, 12057, 12058, 12082, 12104, 12105, 12107, 12113, 12138, 12192, 12205, 12218, 12237, 12249, 12269, 12292, 12299, 12313, 12328, 12329, 12359, 12376, 12435, 12442, 12460, 12464, 12517, 12546, 12568, 12619, 12632, 12640, 12642, 12656, 12659, 12679, 12687, 12692, 12709, 12720, 12732, 12748, 12773, 12817, 12839, 12869, 12880, 12887, 12894, 12909, 12923, 12924, 12930, 12961, 12962, 12973, 12979, 12996, 12997, 12998, 12999, 13009, 13011, 13049, 13051, 13053, 13065, 13089, 13105, 13117, 13121, 13124, 13144, 13219, 13228, 13231, 13233, 13240, 13247, 13253, 13259, 13283, 13285, 13287, 13310, 13315, 13319, 13320, 13322, 13331, 13381, 13457, 13477, 13484, 13495, 13498, 13524, 13527, 13530, 13541, 13561, 13586, 13640, 13647, 13675, 13699, 13704, 13705, 13714, 13738, 13742, 13752, 13759, 13796, 13809, 13844, 13852, 13876, 13901, 13915, 13937, 13962, 13972, 13989, 14026, 14036, 14049, 14068, 14075, 14091, 14121, 14128, 14135, 14153, 14157, 14162, 14164, 14171, 14227, 14258, 14266, 14285, 14297, 14303, 14320, 14332, 14334, 14359, 14369, 14370, 14371, 14382, 14400, 14423, 14490, 14492, 14493, 14501, 14521, 14563, 14568, 14569, 14593, 14600, 14603, 14622, 14626, 14636, 14717, 14750, 14751, 14760, 14783, 14785, 14787, 14804, 14808, 14840, 14858, 14868, 14901, 14938, 14957, 14967, 14968, 14995, 15031, 15034, 15035, 15045, 15064, 15091, 15092, 15115, 15128, 15129, 15171, 15195, 15201, 15203, 15205, 15211, 15213, 15214, 15220, 15231, 15242, 15246, 15248, 15282, 15304, 15313, 15314, 15316, 15350, 15385, 15401, 15405, 15407, 15409, 15421, 15422, 15461, 15463, 15465, 15483, 15490, 15491, 15492, 15497, 15560, 15574, 15616, 15619, 15621, 15636, 15643, 15657, 15659, 15677, 15691, 15706, 15737, 15738, 15754, 15762, 15771, 15785, 15806, 15816, 15818, 15822, 15840, 15857, 15898, 15932, 15944, 15954, 16016, 16022, 16037, 16085, 16126, 16132, 16177, 16187, 16248, 16256, 16280, 16283, 16293, 16299, 16314, 16315, 16327, 16338, 16346, 16350, 16375, 16378, 16398, 16399, 16401, 16419, 16427, 16435, 16437, 16438, 16461, 16477, 16506, 16556, 16563, 16567, 16581, 16636, 16637, 16647, 16662, 16668, 16699, 16706, 16740, 16752, 16763, 16783, 16791, 16793, 16796, 16815, 16848, 16865, 16893, 16911, 16927, 16970, 16974, 16986, 17014, 17020, 17045, 17064, 17073, 17091, 17104, 17113, 17141, 17147, 17162, 17205, 17213, 17218, 17225, 17267, 17358, 17359, 17372, 17375, 17381, 17382, 17402, 17422, 17423, 17429, 17473, 17523, 17525, 17550, 17577, 17604, 17608, 17621, 17630, 17632, 17641, 17661, 17664, 17687, 17698, 17725, 17737, 17751, 17777, 17787, 17825, 17839, 17840, 17887, 17925, 17944, 17949, 17955, 17960, 17982, 18001, 18004, 18040, 18041, 18102, 18104, 18106, 18135, 18176, 18181, 18199, 18209, 18220, 18243, 18288, 18331, 18336, 18337, 18358, 18365, 18387, 18389, 18393, 18399, 18400, 18405, 18413, 18418, 18425, 18432, 18443, 18452, 18458, 18505, 18509, 18518, 18547, 18549, 18565, 18567, 18591, 18593, 18615, 18633, 18649, 18723, 18725, 18738, 18782, 18788, 18807, 18808, 18815, 18821, 18836, 18848, 18876, 18909, 18936, 18988, 18993, 18997, 19016, 19029, 19045, 19063, 19068, 19102, 19123, 19159, 19211, 19227, 19238, 19243, 19249, 19276, 19296, 19313, 19336, 19343, 19361, 19363, 19368, 19425, 19433, 19464, 19484, 19494, 19496, 19527, 19533, 19555, 19582, 19585, 19590, 19592, 19671, 19684, 19718, 19754, 19767, 19772, 19774, 19822, 19825, 19838, 19853, 19887, 19914, 19915, 19961, 19962, 19963, 19968, 19972, 19979, 19982, 20001, 20045, 20074, 20082, 20083, 20098, 20099, 20116, 20164, 20184, 20185, 20213, 20222, 20238, 20250, 20262, 20271, 20285, 20296, 20301, 20307, 20342, 20348, 20393, 20411, 20416, 20428, 20443, 20462, 20465, 20472, 20513, 20521, 20523, 20544, 20558, 20563, 20581, 20592, 20593, 20598, 20618, 20637, 20642, 20648, 20660, 20674, 20698, 20713, 20737, 20747, 20772, 20813, 20824, 20850, 20888, 20908, 20913, 20924, 20925, 20955, 20977, 20985, 20990, 20991, 20997, 21018, 21022, 21028, 21036, 21040, 21048, 21065, 21091, 21100, 21122, 21127, 21130, 21147, 21160, 21199, 21218, 21270, 21282, 21301, 21341, 21351, 21356, 21372, 21375, 21452, 21454, 21475, 21516, 21544, 21547, 21579, 21605, 21641, 21643, 21674, 21778, 21782, 21783, 21813, 21846, 21857, 21862, 21869, 21875, 21896, 21947, 21952, 21959, 21962, 21974, 21979, 21982, 22081, 22084, 22087, 22148, 22172, 22219, 22262, 22275, 22279, 22297, 22328, 22335, 22352, 22452, 22456, 22467, 22506, 22508, 22520, 22531, 22550, 22583, 22634, 22637, 22642]\n",
      "Drop Count: 1356\n",
      "GA fliter Consume: 19680972\n"
     ]
    }
   ],
   "source": [
    "#剪枝\n",
    "import time\n",
    "import ESEA\n",
    "Pruning=True\n",
    "if Pruning:\n",
    "        filter_drop = []\n",
    "        print('Pruning procedure :')\n",
    "        p_size=[1]*22656\n",
    "        ori_acc= float(test(switch=p_size))\n",
    "        st_time = time.time()\n",
    "        ea_helper = ESEA.EA_Util(\n",
    "            'es_ea', \n",
    "            30, \n",
    "            22656, \n",
    "            eval_fun=fitness(ori_acc), \n",
    "            max_gen=100, \n",
    "            drop_set=filter_drop, \n",
    "            sp_set=[],\n",
    "            target='F'\n",
    "        )\n",
    "        elite = ea_helper.evolution()\n",
    "        elite = ea_helper.population[elite]\n",
    "        en_time = time.time()\n",
    "        for x in range(22656):\n",
    "            if elite[x] == 0:\n",
    "                filter_drop.append(x)\n",
    "        print('Drop Set:', filter_drop)\n",
    "        print('Drop Count:', len(filter_drop))\n",
    "        print('GA fliter Consume:', int((en_time-st_time)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a48acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res34  Drop_Set=[12, 37, 50, 63, 64, 71, 76, 84, 90, 100, 117, 129, 138, 144, 146, 150, 153, 155, 165, 175, 208, 212, 227, 263, 300, 313, 314, 331, 342, 348, 355, 359, 362, 364, 393, 395, 423, 424, 437, 439, 458, 465, 479, 483, 495, 529, 539, 555, 565, 595, 598, 603, 611, 648, 672, 673, 674, 691, 693, 697, 700, 704, 710, 712, 720, 730, 747, 748, 764, 766, 770, 773, 783, 794, 809, 817, 840, 854, 858, 860, 864, 878, 883, 886, 914, 917, 920, 978, 982, 992, 993, 1002, 1025, 1044, 1048, 1049, 1052, 1061, 1068, 1087, 1092, 1113, 1123, 1149, 1156, 1174, 1186, 1187, 1188, 1206, 1208, 1233, 1246, 1254, 1291, 1294, 1296, 1303, 1307, 1313, 1328, 1329, 1334, 1338, 1344, 1347, 1354, 1360, 1361, 1364, 1370, 1378, 1392, 1397, 1402, 1414, 1416, 1428, 1434, 1442, 1444, 1445, 1452, 1456, 1485, 1493, 1498, 1503, 1513, 1515, 1523, 1529, 1540, 1545, 1550, 1562, 1570, 1581, 1582, 1583, 1586, 1607, 1619, 1623, 1634, 1656, 1665, 1668, 1670, 1681, 1688, 1716, 1739, 1748, 1763, 1805, 1808, 1809, 1820, 1823, 1836, 1841, 1869, 1870, 1883, 1892, 1904, 1907, 1923, 1935, 1949, 1953, 1979, 1980, 2000, 2002, 2003, 2005, 2009, 2022, 2024, 2026, 2049, 2063, 2069, 2078, 2086, 2101, 2122, 2140, 2159, 2174, 2180, 2193, 2195, 2202, 2217, 2219, 2234, 2243, 2259, 2262, 2263, 2271, 2278, 2301, 2312, 2316, 2317, 2322, 2330, 2331, 2340, 2342, 2352, 2359, 2363, 2375, 2407, 2434, 2448, 2461, 2470, 2471, 2479, 2485, 2489, 2524, 2528, 2561, 2568, 2573, 2577, 2586, 2588, 2590, 2591, 2611, 2621, 2632, 2633, 2639, 2656, 2660, 2677, 2686, 2701, 2706, 2711, 2715, 2720, 2727, 2730, 2744, 2745, 2747, 2761, 2776, 2781, 2784, 2795, 2797, 2824, 2828, 2838, 2879, 2880, 2881, 2897, 2917, 2924, 2927, 2928, 2939, 2969, 2970, 2972, 2975, 2981, 2982, 2984, 3000, 3055, 3077, 3081, 3082, 3113, 3118, 3126, 3127, 3136, 3140, 3144, 3148, 3165, 3167, 3176, 3200, 3207, 3210, 3211, 3212, 3220, 3230, 3246, 3284, 3285, 3291, 3304, 3310, 3313, 3318, 3322, 3336, 3355, 3357, 3372, 3387, 3389, 3410, 3417, 3418, 3422, 3426, 3441, 3444, 3454, 3469, 3470, 3479, 3487, 3501, 3503, 3511, 3513, 3524, 3542, 3556, 3557, 3565, 3575, 3576, 3593, 3597, 3598, 3609, 3620, 3625, 3628, 3634, 3669, 3681, 3684, 3703, 3712, 3719, 3720, 3726, 3728, 3731, 3738, 3743, 3747, 3751, 3758, 3786, 3789, 3792, 3797, 3802, 3804, 3808, 3811, 3812, 3820, 3824, 3828, 3833, 3837, 3841, 3859, 3873, 3875, 3878, 3882, 3883, 3885, 3886, 3889, 3894, 3909, 3924, 3969, 3991, 3995, 3997, 4001, 4024, 4031, 4032, 4035, 4038, 4039, 4049, 4071, 4078, 4084, 4089, 4098, 4106, 4111, 4113, 4117, 4122, 4123, 4124, 4129, 4135, 4136, 4142, 4147, 4164, 4175, 4181, 4199, 4203, 4207, 4211, 4218, 4228, 4232, 4236, 4240, 4241, 4260, 4274, 4277, 4281, 4285, 4288, 4291, 4293, 4294, 4303, 4312, 4328, 4337, 4341, 4342, 4345, 4346, 4353, 4362, 4369, 4371, 4376, 4390, 4406, 4408, 4410, 4418, 4450, 4489, 4491, 4512, 4517, 4525, 4556, 4570, 4579, 4582, 4588, 4589, 4592, 4596, 4598, 4601, 4619, 4625, 4648, 4651, 4656, 4658, 4682, 4686, 4687, 4694, 4699, 4714, 4715, 4720, 4728, 4739, 4742, 4748, 4789, 4801, 4809, 4810, 4814, 4819, 4826, 4837, 4847, 4854, 4858, 4863, 4870, 4878, 4898, 4903, 4904, 4920, 4924, 4930, 4936, 4941, 4970, 4983, 4985, 4995, 5015, 5018, 5047, 5049, 5055, 5065, 5075, 5083, 5084, 5092, 5103, 5104, 5107, 5116, 5121, 5132, 5150, 5169, 5171, 5175, 5185, 5191, 5199, 5220, 5251, 5252, 5264, 5276, 5312, 5320, 5322, 5333, 5347, 5348, 5367, 5368, 5370, 5383, 5388, 5389, 5397, 5435, 5464, 5496, 5512, 5516, 5517, 5527, 5539, 5541, 5556, 5558, 5569, 5581, 5588, 5595, 5597, 5605, 5608, 5659, 5668, 5684, 5688, 5690, 5698, 5699, 5712, 5720, 5736, 5737, 5748, 5759, 5789, 5797, 5809, 5810, 5817, 5831, 5847, 5864, 5882, 5902, 5923, 5940, 5948, 5958, 5964, 5968, 5969, 5990, 5997, 6004, 6014, 6017, 6019, 6020, 6022, 6023, 6025, 6027, 6031, 6035, 6040, 6042, 6057, 6069, 6072, 6078, 6083, 6092, 6109, 6117, 6123, 6130, 6144, 6148, 6156, 6168, 6185, 6197, 6198, 6207, 6224, 6227, 6239, 6240, 6241, 6245, 6256, 6261, 6266, 6314, 6315, 6319, 6322, 6325, 6334, 6352, 6357, 6359, 6368, 6389, 6391, 6393, 6394, 6396, 6398, 6415, 6430, 6440, 6463, 6466, 6470, 6476, 6478, 6497, 6498, 6505, 6513, 6541, 6542, 6548, 6552, 6579, 6597, 6610, 6613, 6623, 6627, 6638, 6667, 6670, 6681, 6696, 6697, 6709, 6740, 6741, 6742, 6754, 6757, 6760, 6774, 6789, 6791, 6800, 6813, 6820, 6831, 6853, 6862, 6864, 6883, 6893, 6935, 6964, 6971, 6973, 6986, 6988, 6998, 7012, 7014, 7041, 7042, 7062, 7069, 7075, 7081, 7085, 7090, 7096, 7105, 7110, 7121, 7128, 7133, 7146, 7147, 7151, 7160, 7169, 7175, 7178, 7193, 7210, 7214, 7219, 7227, 7230, 7234, 7246, 7255, 7257, 7260, 7288, 7298, 7314, 7325, 7346, 7350, 7358, 7368, 7375, 7377, 7381, 7382, 7383, 7389, 7392, 7396, 7397, 7410, 7449, 7454, 7458, 7473, 7499, 7509, 7511, 7520, 7529, 7531, 7533]\n",
    "#best fitness: 1.1195\n",
    "#res50 \n",
    "Drop_Set= [15, 16, 33, 48, 53, 68, 70, 87, 93, 126, 155, 161, 163, 170, 191, 198, 230, 239, 328, 336, 353, 400, 402, 423, 431, 471, 492, 494, 502, 522, 557, 567, 589, 596, 605, 606, 618, 622, 625, 628, 631, 661, 675, 688, 746, 768, 777, 780, 797, 808, 845, 881, 896, 908, 909, 919, 920, 929, 931, 935, 951, 986, 1034, 1047, 1049, 1058, 1079, 1134, 1150, 1222, 1285, 1299, 1328, 1334, 1375, 1384, 1394, 1406, 1420, 1444, 1461, 1478, 1481, 1498, 1579, 1591, 1614, 1620, 1628, 1690, 1733, 1812, 1814, 1825, 1836, 1852, 1858, 1871, 1875, 1881, 1905, 1916, 1929, 1978, 2008, 2036, 2037, 2053, 2056, 2066, 2068, 2151, 2163, 2180, 2184, 2196, 2202, 2213, 2225, 2240, 2261, 2264, 2282, 2288, 2300, 2303, 2304, 2306, 2313, 2322, 2327, 2340, 2345, 2347, 2394, 2409, 2437, 2464, 2496, 2509, 2522, 2530, 2544, 2579, 2587, 2620, 2626, 2632, 2668, 2670, 2679, 2680, 2689, 2695, 2715, 2720, 2722, 2724, 2727, 2745, 2775, 2795, 2797, 2804, 2845, 2855, 2856, 2873, 2907, 2908, 2942, 2948, 2960, 2989, 3018, 3035, 3052, 3059, 3084, 3126, 3150, 3153, 3156, 3161, 3167, 3172, 3194, 3198, 3220, 3250, 3290, 3312, 3324, 3340, 3369, 3375, 3381, 3431, 3437, 3492, 3493, 3521, 3556, 3563, 3571, 3572, 3577, 3579, 3589, 3639, 3649, 3664, 3676, 3702, 3709, 3710, 3728, 3759, 3774, 3780, 3782, 3787, 3789, 3791, 3830, 3849, 3856, 3880, 3899, 3900, 3906, 3912, 3934, 3942, 3947, 3953, 3957, 3972, 3981, 3993, 4006, 4022, 4039, 4103, 4119, 4122, 4130, 4161, 4178, 4187, 4192, 4195, 4205, 4208, 4209, 4222, 4224, 4268, 4272, 4299, 4327, 4330, 4374, 4379, 4386, 4402, 4442, 4457, 4462, 4464, 4487, 4491, 4493, 4497, 4521, 4523, 4524, 4529, 4538, 4559, 4606, 4607, 4622, 4632, 4643, 4660, 4662, 4681, 4683, 4687, 4727, 4764, 4768, 4806, 4807, 4812, 4815, 4823, 4836, 4845, 4856, 4870, 4896, 4925, 4974, 4977, 4989, 4994, 5037, 5054, 5069, 5085, 5088, 5097, 5099, 5114, 5144, 5160, 5174, 5197, 5203, 5237, 5240, 5251, 5258, 5281, 5337, 5375, 5413, 5416, 5444, 5466, 5482, 5485, 5487, 5493, 5512, 5517, 5542, 5564, 5634, 5682, 5705, 5754, 5756, 5761, 5792, 5807, 5816, 5845, 5880, 5911, 5912, 5922, 5941, 5942, 5944, 5956, 6004, 6027, 6034, 6038, 6048, 6054, 6076, 6083, 6087, 6149, 6156, 6161, 6176, 6182, 6225, 6234, 6262, 6281, 6303, 6313, 6360, 6371, 6384, 6389, 6408, 6428, 6445, 6472, 6474, 6490, 6494, 6561, 6599, 6623, 6631, 6640, 6644, 6649, 6664, 6669, 6690, 6709, 6732, 6736, 6741, 6767, 6778, 6794, 6804, 6821, 6834, 6843, 6850, 6865, 6866, 6869, 6884, 6902, 6915, 6923, 6939, 6969, 6971, 6980, 6996, 7074, 7075, 7093, 7136, 7138, 7141, 7151, 7165, 7183, 7194, 7197, 7199, 7205, 7222, 7236, 7247, 7274, 7278, 7294, 7302, 7313, 7325, 7326, 7336, 7340, 7344, 7368, 7376, 7401, 7432, 7446, 7459, 7482, 7484, 7493, 7503, 7558, 7575, 7603, 7612, 7622, 7631, 7641, 7651, 7671, 7677, 7692, 7719, 7729, 7734, 7738, 7747, 7750, 7804, 7806, 7815, 7818, 7828, 7836, 7850, 7895, 7916, 7929, 7959, 7971, 7974, 7988, 7991, 7997, 8053, 8058, 8065, 8075, 8078, 8080, 8088, 8097, 8127, 8130, 8147, 8157, 8167, 8186, 8229, 8274, 8275, 8295, 8301, 8321, 8323, 8333, 8337, 8372, 8375, 8389, 8427, 8433, 8437, 8449, 8450, 8454, 8467, 8477, 8513, 8531, 8532, 8536, 8544, 8567, 8577, 8594, 8628, 8642, 8648, 8652, 8659, 8662, 8682, 8683, 8686, 8688, 8713, 8747, 8751, 8759, 8791, 8798, 8841, 8859, 8867, 8870, 8877, 8880, 8924, 8960, 8966, 8995, 9005, 9017, 9031, 9110, 9117, 9118, 9124, 9133, 9139, 9140, 9150, 9165, 9168, 9173, 9192, 9235, 9269, 9271, 9280, 9287, 9310, 9340, 9379, 9394, 9401, 9411, 9419, 9429, 9478, 9480, 9494, 9519, 9553, 9563, 9574, 9594, 9609, 9644, 9661, 9701, 9718, 9745, 9755, 9760, 9771, 9778, 9783, 9842, 9859, 9873, 9875, 9916, 9948, 9961, 10011, 10017, 10060, 10094, 10110, 10124, 10129, 10134, 10164, 10193, 10195, 10220, 10233, 10242, 10255, 10257, 10303, 10348, 10362, 10368, 10387, 10393, 10409, 10454, 10481, 10491, 10536, 10570, 10580, 10641, 10655, 10679, 10711, 10728, 10744, 10777, 10825, 10838, 10883, 10897, 10898, 10912, 10913, 10929, 10933, 10936, 10945, 10979, 11003, 11026, 11055, 11057, 11058, 11077, 11089, 11111, 11127, 11131, 11179, 11211, 11215, 11232, 11259, 11261, 11267, 11294, 11324, 11346, 11394, 11398, 11402, 11411, 11437, 11465, 11470, 11551, 11592, 11623, 11632, 11642, 11670, 11681, 11695, 11758, 11764, 11769, 11773, 11779, 11780, 11784, 11792, 11798, 11811, 11849, 11850, 11906, 11915, 11923, 11926, 11932, 11964, 11966, 11969, 12018, 12037, 12044, 12047, 12051, 12056, 12057, 12058, 12082, 12104, 12105, 12107, 12113, 12138, 12192, 12205, 12218, 12237, 12249, 12269, 12292, 12299, 12313, 12328, 12329, 12359, 12376, 12435, 12442, 12460, 12464, 12517, 12546, 12568, 12619, 12632, 12640, 12642, 12656, 12659, 12679, 12687, 12692, 12709, 12720, 12732, 12748, 12773, 12817, 12839, 12869, 12880, 12887, 12894, 12909, 12923, 12924, 12930, 12961, 12962, 12973, 12979, 12996, 12997, 12998, 12999, 13009, 13011, 13049, 13051, 13053, 13065, 13089, 13105, 13117, 13121, 13124, 13144, 13219, 13228, 13231, 13233, 13240, 13247, 13253, 13259, 13283, 13285, 13287, 13310, 13315, 13319, 13320, 13322, 13331, 13381, 13457, 13477, 13484, 13495, 13498, 13524, 13527, 13530, 13541, 13561, 13586, 13640, 13647, 13675, 13699, 13704, 13705, 13714, 13738, 13742, 13752, 13759, 13796, 13809, 13844, 13852, 13876, 13901, 13915, 13937, 13962, 13972, 13989, 14026, 14036, 14049, 14068, 14075, 14091, 14121, 14128, 14135, 14153, 14157, 14162, 14164, 14171, 14227, 14258, 14266, 14285, 14297, 14303, 14320, 14332, 14334, 14359, 14369, 14370, 14371, 14382, 14400, 14423, 14490, 14492, 14493, 14501, 14521, 14563, 14568, 14569, 14593, 14600, 14603, 14622, 14626, 14636, 14717, 14750, 14751, 14760, 14783, 14785, 14787, 14804, 14808, 14840, 14858, 14868, 14901, 14938, 14957, 14967, 14968, 14995, 15031, 15034, 15035, 15045, 15064, 15091, 15092, 15115, 15128, 15129, 15171, 15195, 15201, 15203, 15205, 15211, 15213, 15214, 15220, 15231, 15242, 15246, 15248, 15282, 15304, 15313, 15314, 15316, 15350, 15385, 15401, 15405, 15407, 15409, 15421, 15422, 15461, 15463, 15465, 15483, 15490, 15491, 15492, 15497, 15560, 15574, 15616, 15619, 15621, 15636, 15643, 15657, 15659, 15677, 15691, 15706, 15737, 15738, 15754, 15762, 15771, 15785, 15806, 15816, 15818, 15822, 15840, 15857, 15898, 15932, 15944, 15954, 16016, 16022, 16037, 16085, 16126, 16132, 16177, 16187, 16248, 16256, 16280, 16283, 16293, 16299, 16314, 16315, 16327, 16338, 16346, 16350, 16375, 16378, 16398, 16399, 16401, 16419, 16427, 16435, 16437, 16438, 16461, 16477, 16506, 16556, 16563, 16567, 16581, 16636, 16637, 16647, 16662, 16668, 16699, 16706, 16740, 16752, 16763, 16783, 16791, 16793, 16796, 16815, 16848, 16865, 16893, 16911, 16927, 16970, 16974, 16986, 17014, 17020, 17045, 17064, 17073, 17091, 17104, 17113, 17141, 17147, 17162, 17205, 17213, 17218, 17225, 17267, 17358, 17359, 17372, 17375, 17381, 17382, 17402, 17422, 17423, 17429, 17473, 17523, 17525, 17550, 17577, 17604, 17608, 17621, 17630, 17632, 17641, 17661, 17664, 17687, 17698, 17725, 17737, 17751, 17777, 17787, 17825, 17839, 17840, 17887, 17925, 17944, 17949, 17955, 17960, 17982, 18001, 18004, 18040, 18041, 18102, 18104, 18106, 18135, 18176, 18181, 18199, 18209, 18220, 18243, 18288, 18331, 18336, 18337, 18358, 18365, 18387, 18389, 18393, 18399, 18400, 18405, 18413, 18418, 18425, 18432, 18443, 18452, 18458, 18505, 18509, 18518, 18547, 18549, 18565, 18567, 18591, 18593, 18615, 18633, 18649, 18723, 18725, 18738, 18782, 18788, 18807, 18808, 18815, 18821, 18836, 18848, 18876, 18909, 18936, 18988, 18993, 18997, 19016, 19029, 19045, 19063, 19068, 19102, 19123, 19159, 19211, 19227, 19238, 19243, 19249, 19276, 19296, 19313, 19336, 19343, 19361, 19363, 19368, 19425, 19433, 19464, 19484, 19494, 19496, 19527, 19533, 19555, 19582, 19585, 19590, 19592, 19671, 19684, 19718, 19754, 19767, 19772, 19774, 19822, 19825, 19838, 19853, 19887, 19914, 19915, 19961, 19962, 19963, 19968, 19972, 19979, 19982, 20001, 20045, 20074, 20082, 20083, 20098, 20099, 20116, 20164, 20184, 20185, 20213, 20222, 20238, 20250, 20262, 20271, 20285, 20296, 20301, 20307, 20342, 20348, 20393, 20411, 20416, 20428, 20443, 20462, 20465, 20472, 20513, 20521, 20523, 20544, 20558, 20563, 20581, 20592, 20593, 20598, 20618, 20637, 20642, 20648, 20660, 20674, 20698, 20713, 20737, 20747, 20772, 20813, 20824, 20850, 20888, 20908, 20913, 20924, 20925, 20955, 20977, 20985, 20990, 20991, 20997, 21018, 21022, 21028, 21036, 21040, 21048, 21065, 21091, 21100, 21122, 21127, 21130, 21147, 21160, 21199, 21218, 21270, 21282, 21301, 21341, 21351, 21356, 21372, 21375, 21452, 21454, 21475, 21516, 21544, 21547, 21579, 21605, 21641, 21643, 21674, 21778, 21782, 21783, 21813, 21846, 21857, 21862, 21869, 21875, 21896, 21947, 21952, 21959, 21962, 21974, 21979, 21982, 22081, 22084, 22087, 22148, 22172, 22219, 22262, 22275, 22279, 22297, 22328, 22335, 22352, 22452, 22456, 22467, 22506, 22508, 22520, 22531, 22550, 22583, 22634, 22637, 22642]\n",
    "#Best Fitness: 1.1184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671a703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): ModuleList(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (classifer): Linear(in_features=2048, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "\n",
    "net=torch.load(\"model/ResNet50.pth\")\n",
    "\n",
    "net=net.module\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "\n",
    "net.to(device)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d317957",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list=[]\n",
    "for i in range(0,22656):\n",
    "    if(i in Drop_Set):\n",
    "        drop_list.append(0)\n",
    "    if(i not in Drop_Set):\n",
    "        drop_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6129ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9093, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(drop_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
